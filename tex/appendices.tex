% !TEX program = optex
% !TEX root = ambrojak-thesis-masters.tex
\app Other Figures

\medskip
\clabel[distributionRefIm]{Distribution of Additional Reference (import)}
\picw=15cm \cinspic imgs/Distribution-Additional-reference-import.pdf
\caption/f Distribution of presence of Additional Reference (import)
\medskip

\medskip
\clabel[distributionChassisIm]{Distribution of Chassis (import)}
\picw=15cm \cinspic imgs/Distribution-Chassis-type-import.pdf
\caption/f Distribution of Chassis Type (import)
\medskip

\label[src-snippets]
\app Additional source code snippets
\rfc{ADD github link}
Some key files are present below, the full source codes is in GitHub: TODO.

\sec pipeline_utils.py

\begtt \hisyntax{Python}
# -*- coding: utf-8 -*-
import time
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB, ComplementNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import VotingClassifier
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from src.preprocessors.text_preprocessor import TextPreprocessor, PrinterNonTransformer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
# scikeras needs scikit-learn version 1.5.2 (default 1.6.1 throws: 'super' object has no attribute '__sklearn_tags__'.
from scikeras.wrappers import KerasClassifier
from tensorflow import keras
from tensorflow.keras.layers import Dense, Dropout, Input
from sklearn.metrics import accuracy_score
import pandas as pd
from plot_utils import plot_score_distribution, plot_roc_curve, plot_precision_recall_curve


def build_model(meta):
    input_dim = meta['n_features_in_']
    num_output_classes = meta['n_classes_']
    model = keras.Sequential([
        Input(shape=(input_dim,)),
        Dense(64, activation='relu'),
        Dropout(0.1),
        Dense(32, activation='relu'),
        Dense(num_output_classes,
              activation='softmax' if num_output_classes > 1 else 'sigmoid')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model


def print_anomaly_confusion_table(ok_data_len, ok_data_predicted_ok, anomalous_data_len, anomalous_data_predicted_anomalous):
    # Compute confusion matrix values
    TP = anomalous_data_predicted_anomalous
    FN = anomalous_data_len - TP
    TN = ok_data_predicted_ok
    FP = ok_data_len - TN

    # Count table
    print("Confusion Matrix (Counts):")
    print(f"{'is_anomaly/predicted':<20} {'True':>8} {'False':>8}")
    print(f"{'True':<20} {TP:8} {FN:8}")
    print(f"{'False':<20} {FP:8} {TN:8}")

    # Row-wise percentages
    print("\nConfusion Matrix (Row-wise %):")
    print(f"{'is_anomaly/predicted':<20} {'True':>8} {'False':>8}")
    print(f"{'True':<20} {TP/anomalous_data_len:8.2%} {FN/anomalous_data_len:8.2%}")
    print(f"{'False':<20} {FP/ok_data_len:8.2%} {TN/ok_data_len:8.2%}")

    print("\n\n")

    return TP, FP, TN, FN


def create_pipelines(X_cols, estimators=None):
    """
    Creates a pipelines with text preprocessing and the given estimators.
    """
    if not estimators:
        estimators = [
            ("LogRegression", LogisticRegression()),
            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
            ('xgb', XGBClassifier(use_label_encoder=False,
             eval_metric='logloss', random_state=42)),
            ('lgbm', LGBMClassifier(use_label_encoder=False,
             random_state=42, verbosity=-1)),
            # ('svc', SVC(probability=True, random_state=42)),  # probability=True enables predict_proba; Probably takes the longest time
            # needs scikit-learn version 1.5.2 (default 1.6.1 throws 'super' object has no attribute '__sklearn_tags__'.
            ('KerasNNClassifier', KerasClassifier(
                model=build_model, epochs=5, verbose=0)),
        ]
    voting = VotingClassifier(
        estimators=estimators,
        voting="soft",
    )

    cols = [('BagOfWords' + col, TfidfVectorizer(), col) for col in X_cols]
    basic_pipeline = [
        ('TextPreprocessor', TextPreprocessor(X_cols)),
        ('VectorizeText', ColumnTransformer(cols)),
    ]
    pipelines = []
    for estimator in estimators + [('VotingClassifier', voting)]:
        pipelines.append(Pipeline(basic_pipeline + [estimator]))
    return pipelines


def create_pipelines_for_probs(X_cols, estimators=None):
    """
    Creates a pipelines with text preprocessing and the given estimators.
    """
    if not estimators:
        estimators = [
            # probability=True enables predict_proba;
            ("LogRegression", LogisticRegression()),
            # ("LogRegressionSagaL1", LogisticRegression(solver='saga', penalty='l1', C=1.0, max_iter=1000)),
            # ("LogRegressionSagaL2", LogisticRegression(solver='saga', penalty='l2', C=1.0, max_iter=1000)),
            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
            ('sgdLogLoss', SGDClassifier(loss='log_loss', random_state=42)),
            ('sgdModifiedHuber', SGDClassifier(
                loss='modified_huber', random_state=42)),
            # SCV Probably takes the longest time
            # ('svc', SVC(probability=True, random_state=42)),
            ('ComplementNB', ComplementNB()),
            ('MultinomialNB', MultinomialNB()),
            ('BernoulliNB', BernoulliNB()),
            ('KNeighborsClassifier', KNeighborsClassifier()),
            # ('QuadraticDiscriminantAnalysis',QuadraticDiscriminantAnalysis()), # Does not make sense in high dim sparse data
        ]
    voting = VotingClassifier(
        estimators=estimators,
        voting="soft",
    )

    cols = [('BagOfWords' + col, TfidfVectorizer(), col) for col in X_cols]
    basic_pipeline = [
        ('TextPreprocessor', TextPreprocessor(X_cols)),
        ('VectorizeText', ColumnTransformer(cols)),
    ]
    pipelines = []
    for estimator in estimators + [('VotingClassifier', voting)]:
        pipelines.append(Pipeline(basic_pipeline + [estimator]))
    return pipelines


def evaluate_pipeline_from_probs(pipe, X_train, y_train, X_test, y_test, threshold=0.6, y_anomalous=None, target_col=None):
    """
    Fits the pipeline, predicts, and evaluates accuracy and timing.
    """
    # Time the fit
    start_fit = time.time()
    pipe.fit(X_train, y_train)
    fit_time = time.time() - start_fit

    # Time the predict
    start_pred = time.time()
    probs = pipe.predict_proba(X_test)
    pred_time = time.time() - start_pred

    # print("Classifier knows classes:", pipe.classes_)
    # print("y_test unique classes:", np.unique(y_test))
    # true_class_probs = probs[np.arange(len(y_test)), y_test]
    # preds = true_class_probs < threshold
    # Above code does not handle classes not seen in data
    # Code belowe does:
    class_indices = {label: i for i, label in enumerate(pipe.classes_)}
    true_class_probs = np.array([
        probs[i, class_indices[y]] if y in class_indices else np.nan
        for i, y in enumerate(y_test)
    ])
    preds = true_class_probs <= threshold

    ok_data_len = len(y_test)
    ok_data_predicted_ok = len(y_test) - sum(preds)

    if not y_anomalous:
        labels = y_test.unique()
        mapping = create_shuffled_mapping(labels)
        y_anomalous = y_test.map(mapping)

    class_indices = {label: i for i, label in enumerate(pipe.classes_)}
    true_class_probs_anomaly = np.array([
        probs[i, class_indices[y]] if y in class_indices else np.nan
        for i, y in enumerate(y_anomalous)
    ])
    preds = true_class_probs_anomaly <= threshold

    scores = np.concatenate([true_class_probs, true_class_probs_anomaly])
    y_true = np.concatenate([np.zeros(len(true_class_probs)), np.ones(len(true_class_probs_anomaly))])
    plot_roc_curve(y_true, scores)
    plot_precision_recall_curve(y_true, scores)
    plot_score_distribution(scores, bins=50)
    
    
    plot_score_distribution

    anomalous_data_len = len(y_anomalous)
    anomalous_data_predicted_anomalous = sum(preds)

    print("Estimator:", pipe.steps[-1][0])
    print(f"Fit time: {fit_time:.3f}s | Predict time: {pred_time:.3f}s")
    TP, FP, TN, FN = print_anomaly_confusion_table(
        ok_data_len, ok_data_predicted_ok, anomalous_data_len, anomalous_data_predicted_anomalous)
    return TP, FP, TN, FN


def create_shuffled_mapping(categories):
    """
    Creates a shuffled mapping for anomaly detection.
    """
    shuffled = categories.copy()
    if len(categories) <= 1:
        return dict(zip(categories, categories))
    while True:
        np.random.shuffle(shuffled)
        if not np.any(shuffled == categories):
            break
    return dict(zip(categories, shuffled))


def evaluate_pipeline(pipe, X_train, y_train, X_test, y_test, y_anomalous=None, verbose=False):
    """
    Fits the pipeline, predicts, and evaluates accuracy and timing.
    """
    # Time the fit
    start_fit = time.time()
    pipe.fit(X_train, y_train)
    fit_time = time.time() - start_fit

    # Time the predict
    start_pred = time.time()
    y_pred = pipe.predict(X_test)
    pred_time = time.time() - start_pred

    if not y_anomalous:
        labels = y_test.unique()
        mapping = create_shuffled_mapping(labels)
        y_anomalous = y_test.map(mapping)

    ok_data_len = anomalous_data_len = len(y_test)
    y_ok_equal = (np.array(y_test) == np.array(y_pred))
    ok_data_predicted_ok = sum(y_ok_equal)
    y_anomalous_not_equal = (np.array(y_pred) != np.array(y_anomalous))
    anomalous_data_predicted_anomalous = sum(y_anomalous_not_equal)

    print("Estimator:", pipe.steps[-1][0])
    print(f"Fit time: {fit_time:.3f}s | Predict time: {pred_time:.3f}s")
    TP, FP, TN, FN = print_anomaly_confusion_table(
        ok_data_len, ok_data_predicted_ok, anomalous_data_len, anomalous_data_predicted_anomalous)
    return TP, FP, TN, FN
    # Accuracy
    # print("Estimator:", pipe.steps[-1][0])
    acc = accuracy_score(y_test, y_pred)
    # print(f"OK data predicted as ok: {acc:.3f}")
    if y_anomalous is not None:
        acc_anomalous = accuracy_score(y_pred, y_anomalous)
    #     print(f"Anomalous data predicted as ok: {acc_anomalous:.3f}")
    # print(f"Fit time: {fit_time:.3f}s | Predict time: {pred_time:.3f}s")

    # Show some examples
    correct = []
    incorrect = []
    if isinstance(pipe.steps[-1][1], VotingClassifier) and verbose:
        for i in range(len(y_test[:10000])):
            true = y_test.iloc[i]
            pred = y_pred[i]
            if pred == true:
                correct.append((i, true, pred))
            else:
                incorrect.append((i, true, pred))

        print("\nExamples of correct predictions:")
        for i, true, pred in correct[:5]:
            print(f"{X_test.iloc[i]}: True = {true}, Predicted = {pred}")

        print("\nExamples of incorrect predictions:")
        for i, true, pred in incorrect[:5]:
            print(f"{X_test.iloc[i]}: True = {true}, Predicted = {pred}")


def evaluate_pipeline_oodd(pipe, X_train, X_test, X_anomalous=None, target_col=None, type='continuous'):
    """
    Fits the pipeline, predicts, and evaluates accuracy and timing.
    """
    # Time the fit
    start_fit = time.time()
    pipe.fit(X_train)
    fit_time = time.time() - start_fit

    # Time the predict
    start_pred = time.time()
    preds = pipe.predict(X_test)
    pred_time = time.time() - start_pred

    ok_data_len = len(preds)
    ok_data_predicted_ok = len(preds) - sum(preds)

    if not X_anomalous and target_col:
        X_anomalous = X_test.copy()
        if type == 'categorical':
            labels = X_anomalous[target_col].unique()
            print(labels)
            mapping = create_shuffled_mapping(labels)
            X_anomalous[target_col] = X_anomalous[target_col].map(mapping)
        elif type == 'categorical3':
            X_anomalous[target_col] = (X_anomalous[target_col] + 1) % 3 + 1
        elif type == 'categorical2':
            X_anomalous[target_col] = (X_anomalous[target_col] + 1) % 2
        elif type == 'continuous':
            X_anomalous[target_col] = X_anomalous[target_col].astype(int) + \
                np.random.choice([20, -20], size=len(X_anomalous))
        else:
            print("Testing Unexpected type of problem")

        # Time the predict
    start_pred = time.time()
    preds = pipe.predict(X_anomalous)
    pred_time = time.time() - start_pred

    anomalous_data_len = len(preds)
    anomalous_data_predicted_anomalous = sum(preds)

    # print("Estimator:", pipe.steps[-1][0])
    print(f"Fit time: {fit_time:.3f}s | Predict time: {pred_time:.3f}s")
    TP, FP, TN, FN = print_anomaly_confusion_table(
        ok_data_len, ok_data_predicted_ok, anomalous_data_len, anomalous_data_predicted_anomalous)
    return TP, FP, TN, FN

\endtt

\sec target_preprocessor.py

\begtt \hisyntax{Python}
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
import pandas as pd
import numpy as np
from text_preprocessor import TextPreprocessor
from typing import Union, Optional


def create_target_pipeline(
    target_col: str,
    anomaly_type: str,
    text_preprocessor: bool = True,
    label_encoder: bool = True
) -> Pipeline:
    """
    Create a preprocessing pipeline for the target column.

    Args:
        target_col (str): The name of the target column to preprocess.
        anomaly_type (str): The type of anomaly being processed, which determines 
                            specific preprocessing logic in the pipeline.
        text_preprocessor (bool, optional): Whether to include a text preprocessor 
                                            in the pipeline. Defaults to True.
        label_encoder (bool, optional): Whether to include a label encoder in the 
                                        pipeline. Defaults to True.

    Returns:
        Pipeline: A scikit-learn Pipeline object that applies the specified 
                  preprocessing steps to the target column.
    """
    pipe = []
    pipe.append(
        ('target_preprocessor', TargetColumnPreprocessor(target_col, anomaly_type)))
    if text_preprocessor:
        pipe.append(
            ('text_preprocessor', TextPreprocessor(columns=[target_col])))
    if label_encoder:
        pipe.append(
            ('label_encoder', DataFrameLabelEncoder(target_col=target_col)))
    return Pipeline(pipe)


class TargetColumnPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self, target_col: str, anomaly_type: str) -> None:
        """
        Initialize the preprocessor with the target column and anomaly type.

        Args:
            target_col (str): The name of the target column.
            anomaly_type (str): The type of anomaly being processed.
        """
        self.target_col = target_col
        self.anomaly_type = anomaly_type

    def fit(self, X: Union[pd.DataFrame, pd.Series], y: Optional[pd.Series] = None) -> "TargetColumnPreprocessor":
        return self

    def transform(self, X: Union[pd.DataFrame, pd.Series]) -> Union[pd.DataFrame, pd.Series]:
        """
        Transform the input DataFrame or Series.

        Args:
            X (pd.DataFrame or pd.Series): The input data.

        Returns:
            pd.DataFrame or pd.Series: The transformed data.
        """
        if isinstance(X, pd.Series):
            X = X.to_frame(name=self.target_col)

        X = X.copy()

        if self.target_col in ["B_NUMER", "Z_SPEDITER", "IM_TK_HEAT"]:
            X[self.target_col] = X[self.target_col].map(
                {None: 0.0, ' ': 0.0, 'X': 1.0}).fillna(X[self.target_col])
            X[self.target_col] = pd.to_numeric(
                X[self.target_col], errors='coerce')

        elif self.target_col == "HEATING_TYPE":
            if "WITH_NULLS" in self.anomaly_type:
                X[self.target_col] = pd.to_numeric(
                    X[self.target_col], errors='coerce').fillna(0.0)
            else:
                X[self.target_col] = pd.to_numeric(
                    X[self.target_col], errors='coerce')

        elif self.target_col in ["EX_CO_TYP", "IM_CO_TYP", "T_CH_TYP", "HEATING_NOTE"]:
            X[self.target_col] = X[self.target_col].astype(str)

        elif self.target_col in ["TEMP_FROM", "TEMP_TO", "TEMPERATURE"]:
            X[self.target_col] = X[self.target_col].apply(lambda x: int(str(x).replace("°", "").replace("C", "").replace('+','').strip()) 
                    if pd.notnull(x) and str(x).replace("°", "").replace("C", "").replace('+','').strip().isdigit() 
                    else -1000 # np.nan # causes problems later, create dummy value that is far away from everything
                    )
            X[self.target_col] = pd.to_numeric(
                X[self.target_col], errors='coerce')

        elif self.target_col in ["IM_SP_REF", "EX_SP_REF"]:
            X[self.target_col] = X[self.target_col].astype(str)
            X[self.target_col] = X[self.target_col].apply(
                lambda x: 0 if pd.isna(x) or str(x).strip() == '' else 1)
            # X[self.target_col] = pd.to_numeric(
            #     X[self.target_col], errors='coerce')

        return X if isinstance(X, pd.DataFrame) else X[self.target_col]


class DataFrameLabelEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, target_col: str) -> None:
        """
        Initialize the transformer with the target column.

        Args:
            target_col (str): The name of the target column to encode.
        """
        self.target_col = target_col
        self.label_encoder = LabelEncoder()

    def fit(self, X: Union[pd.DataFrame, pd.Series], y: Optional[pd.Series] = None) -> "DataFrameLabelEncoder":
        """
        Fit the LabelEncoder to the target column.

        Args:
            X (pd.DataFrame or pd.Series): The input data.
            y: Ignored.

        Returns:
            self
        """
        if isinstance(X, pd.Series):
            X = X.to_frame(name=self.target_col)

        if self.target_col in X.columns:
            self.label_encoder.fit(X[self.target_col].astype(str))
        return self

    def transform(self, X: Union[pd.DataFrame, pd.Series]) -> Union[pd.DataFrame, pd.Series]:
        """
        Transform the target column using the fitted LabelEncoder.

        Args:
            X (pd.DataFrame or pd.Series): The input data.

        Returns:
            pd.DataFrame or pd.Series: The transformed data.
        """
        if isinstance(X, pd.Series):
            X = X.to_frame(name=self.target_col)

        X = X.copy()
        if self.target_col in X.columns:
            X[self.target_col] = self.label_encoder.transform(
                X[self.target_col].astype(str))

        return X if isinstance(X, pd.DataFrame) else X[self.target_col]

    def inverse_transform(self, X: Union[pd.DataFrame, pd.Series]) -> Union[pd.DataFrame, pd.Series]:
        """
        Inverse transform the target column to its original values.

        Args:
            X (pd.DataFrame or pd.Series): The input data.

        Returns:
            pd.DataFrame or pd.Series: The data with the target column inverse transformed.
        """
        if isinstance(X, pd.Series):
            X = X.to_frame(name=self.target_col)

        X = X.copy()
        if self.target_col in X.columns:
            X[self.target_col] = self.label_encoder.inverse_transform(
                X[self.target_col])

        return X if isinstance(X, pd.DataFrame) else X[self.target_col]

\endtt

\sec text_preprocessor.py

\begtt \hisyntax{Python}
import unicodedata
import re
from sklearn.base import BaseEstimator, TransformerMixin


class TextPreprocessor(TransformerMixin, BaseEstimator):
    def __init__(self, columns):
        self.columns = columns

    def fit(self, X, y=None):
        return self  # stateless

    def transform(self, X):
        X_copy = X.copy()
        for col in self.columns:
            X_copy[col] = X_copy[col].astype(str)
            X_copy[col] = X_copy[col].apply(self.preprocess_text)
        return X_copy

    def preprocess_text(self, text):
        # Normalize and replace accented chars with ASCII equivalents
        text = unicodedata.normalize('NFKD', text)
        text = text.encode('ascii', 'ignore').decode('ascii')
        text = re.sub(r'\s', ' ', text)
        text = text.upper()
        text = re.sub(r'[^A-Z0-9]', ' ', text)
        text = re.sub(r' +', ' ', text)
        return text.strip()
\endtt

\sec counter_OODD.py
These classes are my own simple models compatible wih skelarn pipeline that are implementing FBOD and FBOD with Back-off smoothing repspectively

\begtt \hisyntax{Python}
from collections import defaultdict, Counter
from sklearn.base import BaseEstimator, ClassifierMixin
import numpy as np
import pandas as pd

class CountBasedClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, X_cols, target_col, threshold=0.1):
        """
        X_cols: list of feature‐column names to count on
        target_col: name of the target/class column
        threshold: min prob to output a class (else None)
        """
        self.X_cols = X_cols
        self.target_col = target_col
        self.threshold = threshold

    def fit(self, X, y=None):
        """
        X: pandas DataFrame (must contain both X_cols and target_col)
        y: ignored (we pull target from X[self.target_col])
        """
        # ensure DataFrame
        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X, columns=self.X_cols + [self.target_col])

        # keep track of all seen classes
        self.classes_ = np.unique(X[self.target_col].values)

        # nested dict: feature‐tuple → Counter of classes
        self.counts_ = defaultdict(Counter)
        for _, row in X.iterrows():
            key = tuple(row[col] for col in self.X_cols)
            cls = row[self.target_col]
            self.counts_[key][cls] += 1

        # also store total counts per key for fast lookup
        self.totals_ = {k: sum(cnt.values()) for k, cnt in self.counts_.items()}
        return self

    def predict_proba(self, X):
        """
        Returns an array of shape (n_samples, n_classes)
        where each entry is:
          count((x),c) / (total_count((x)) + 1.0)
        The order of classes is given by self.classes_
        """
        # Convert to DataFrame if needed
        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X, columns=self.X_cols)

        n = X.shape[0]
        m = len(self.classes_)
        proba = np.zeros((n, m), dtype=float)

        for i, (_, row) in enumerate(X.iterrows()):
            key = tuple(row[col] for col in self.X_cols)
            counter = self.counts_.get(key, Counter())
            total = self.totals_.get(key, 0)

            for j, cls in enumerate(self.classes_):
                cnt = counter.get(cls, 0)
                # denominator smoothing +1.0
                proba[i, j] = cnt / (total + 1.0)

        return proba

    def predict(self, X):
        """
        Returns True if P(true class | X) < threshold (i.e. anomalous),
        else False.
        Assumes X includes the target_col.
        """
        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X, columns=self.X_cols + [self.target_col])

        proba = self.predict_proba(X[self.X_cols])
        result = []

        for i, true_class in enumerate(X[self.target_col]):
            if true_class not in self.classes_:
                result.append(True)  # unknown class = suspicious
                continue
            idx = np.where(self.classes_ == true_class)[0][0]
            prob = proba[i, idx]
            result.append(prob < self.threshold)

        return np.array(result)

class FallbackCountClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, X_cols, target_col, threshold=0.1):
        """
        X_cols: list of feature‐column names (e.g. ['f1','f2'])
        target_col: name of the class column
        threshold: P(true class|X) cutoff for 'anomaly' (True = anomaly)
        """
        self.X_cols = X_cols
        self.target_col = target_col
        self.threshold = threshold
        
        self.fallback_combos = []
        # 1. Full combo
        self.fallback_combos.append(tuple(X_cols))
        # 2. First two as pair, if applicable
        if len(X_cols) > 2:
            self.fallback_combos.append(tuple(X_cols[:2]))
        # 3. Each of the first 5 columns individually
        for col in X_cols[:5]:
            if (col,) not in self.fallback_combos:
                self.fallback_combos.append((col,))
        print(self.fallback_combos)

    def fit(self, X, y=None):
        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X.copy()
        # record classes
        self.classes_ = np.unique(X[self.target_col])

        # build counts & totals for each combo
        self.counts_ = {combo: defaultdict(Counter) 
                        for combo in self.fallback_combos}
        self.totals_ = {combo: defaultdict(int) 
                        for combo in self.fallback_combos}

        for _, row in X.iterrows():
            cls = row[self.target_col]
            for combo in self.fallback_combos:
                key = tuple(row[c] for c in combo)
                self.counts_[combo][key][cls] += 1
                self.totals_[combo][key] += 1

        return self

    def predict_proba(self, X):
        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X
        n, m = X.shape[0], len(self.classes_)
        proba = np.zeros((n, m), float)

        for i, (_, row) in enumerate(X.iterrows()):
            # find first combo with data
            for combo in self.fallback_combos:
                key = tuple(row[c] for c in combo)
                total = self.totals_[combo].get(key, 0)
                if total > 0:
                    counter = self.counts_[combo][key]
                    for j, cls in enumerate(self.classes_):
                        cnt = counter.get(cls, 0)
                        proba[i, j] = cnt / (total + 1.0)
                    break
            # else: proba[i, :] stays zeros

        return proba

    def predict(self, X):
        """
        Returns True = anomalous (P(true class) < threshold)
                False = confident (P >= threshold)
        Expects X with both features and target_col.
        """
        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X
        proba = self.predict_proba(X[self.X_cols])
        flags = []
        for i, true_cls in enumerate(X[self.target_col]):
            if true_cls not in self.classes_:
                flags.append(True)
                continue
            idx = np.where(self.classes_ == true_cls)[0][0]
            flags.append(proba[i, idx] < self.threshold)
        return np.array(flags)
\endtt