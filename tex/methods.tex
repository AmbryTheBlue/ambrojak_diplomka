% !TEX program = optex
% !TEX root = ambrojak-thesis-masters.tex
\label[methods]
\chap Methods for Anomaly Detection
This theoretical chapter starts with a definition of Anomaly Detection and introduces the difference between supervised, semi-supervised and unsupervised problems. Afterwards a brief introduction to several methods typically associated with anomaly detection is given. They are split into statistical, clustering, tree-based, and deep learning sections. We end this chapter with discussion of which of these seems most appropriate for our data.


\sec Introduction to Anomaly Detection
Anomaly detection (sometimes shortened as \glref{AD}), also referred to as outlier detection (sometimes abbreviated as \glref{OD}), is the task of identifying instances in a dataset that deviate significantly from the majority of the data. These deviations, termed anomalies, often represent rare or unusual events, errors, or significant phenomena of interest. Applications of anomaly detection span diverse fields, including fraud detection, system monitoring, medical diagnostics, and natural language processing. \cite[chandola2009]

\secc Supervised Anomaly Detection
In supervised anomaly detection, models are trained on fully labeled datasets that contain examples of normal and anomalous. A classifier learns to distinguish between the two classes and flags deviations accordingly.
Although this approach can achieve high accuracy for known anomaly types, it requires extensive labeled data, which is often impractical, because anomalies are, by definition, rare and expensive to label. \cite[ibmAnomalyDetection]

\secc Semi-Supervised Anomaly Detection
Semi-supervised methods leverage a small quantity of labeled data (typically only normal instances) alongside a large pool of unlabeled data to build a model of normal behavior and detect deviations.
These techniques are particularly useful when anomalous labels are scarce but normal examples are abundant. By modeling only the “normal” class, they flag instances that do not fit this pattern as anomalies.\cite[ibmAnomalyDetection]


\secc Unsupervised Anomaly Detection
Unsupervised anomaly detection is particularly relevant in cases where labeled data (that is, explicit examples of anomalous and normal instances) are unavailable. In this paradigm, the algorithm assumes that anomalies are rare and distinct from the ``normal'' data. The approach involves modeling the patterns, distributions, or structure of normal data and identifying data points that deviate significantly from these learned patterns.\cite[unsupervisedAnomalyDetection]

This type of problem describes scenarios where only examples of normal behavior are available. It is necessary to use techniques such as clustering, reconstruction errors, distance measures, or density estimation; unsupervised anomaly detection provides a robust framework for identifying unexpected patterns without relying on prior knowledge of what constitutes an anomaly.\cite[unsupervisedAnomalyDetection]

\secc Summary
In summary, the choice among supervised, semi-supervised, and unsupervised anomaly detection depends primarily on the availability of labeled examples of anomalous behavior and the expected complexity of normal data distributions.

In the next section, we will survey specific algorithmic families suited to different data characteristics, before selecting those most appropriate for the Metrans transport order dataset.

\sec Statistical Approaches to Anomaly Detection

Statistical methods for anomaly detection are based on the assumption that the data follow a specific distribution or statistical model. Anomalies are identified as points that deviate significantly from this assumed distribution or exhibit rare or improbable characteristics. These methods are particularly effective when the underlying data structure is well understood and the anomalies are clearly distinguishable from normal variation.\cite[chandola2009]

\secc Parametric Approaches
Parametric anomaly-detection techniques posit that normal data are drawn from a member of a known distributional family (most commonly Gaussian, Poisson, or exponential) and then quantify how improbable each observation is under maximum-likelihood parameter estimates. A prototypical example is the {\bf Z-score test}, which flags points whose standardized distance from the sample mean exceeds a user-defined threshold (e.g., |z| > 3) when normality holds. Formal single-point hypothesis tests such as the {\bf Grubbs test} or the generalized extreme studentized deviation extend this idea to small samples and heavy-tailed distributions by comparing each candidate point to critical values of the assumed model. Although these methods are computationally simple and interpretable, large benchmark studies demonstrate that their accuracy drops sharply when the true data-generating process is multimodal or skewed, highlighting their sensitivity to model misspecification \cite[outlierSurvey, chandola2009]

Anomaly detection based on {\bf PDE} ({\bf Probability Density Estimation})first fits a parametric distribution to the 'normal' data, most commonly by maximum likelihood estimation of a Gaussian, Poisson, or (in low-dimensional mixtures) a finite-mixture model. Each observation $x_i$ is then evaluated under the fitted density $p_θ(x)$; points whose log-likelihood falls below a significance threshold (or equivalently whose tail probability is smaller than $\alpha$ are flagged as anomalies. Because the entire decision rule is derived from the assumed form of $p_θ$. PDE yields interpretable probabilistic scores and tight control over the false-alarm rate when the model specification is valid. Its chief weakness is model misspecification: multimodal, skewed, or heavy-tailed real-world data can cause likelihoods to be badly calibrated, leading to false positives and false negatives, a phenomenon extensively documented in comparative surveys of outlier detection algorithms. \cite[chandola2009]


The {\bf Chi-Square test} is a non-parametric statistical method used to determine whether there is a significant association between two categorical variables. It compares the observed frequencies in each category of a contingency table with the frequencies expected under the assumption of independence. The test statistic is calculated as:
$$
\chi^2 = \sum {(O-E)^2 \over E}
$$
where $O$ represents the observed frequency and $E$ denotes the expected frequency under the null hypothesis of independence. A large Chi-Square statistic indicates a significant difference between observed and expected frequencies, suggesting a potential association between the variables.\cite[chi-square]

In the context of anomaly detection, the Chi-Square test can be applied to identify unusual patterns in categorical data. For instance, it can detect deviations in user behavior on a website by comparing current page visit frequencies to historical norms. Significant differences may indicate anomalies such as fraudulent activities or system malfunctions.\cite[chi-square]

However, the Chi-Square test has limitations. It requires a sufficient sample size to ensure the validity of the approximation to the Chi-Square distribution. Specifically, expected frequencies in each cell of the contingency table should generally be at least 5. When this condition is not met, the test's reliability diminishes, and alternative methods, such as Fisher's Exact Test, may be more appropriate.\cite[chi-square]


\secc Non-Parametric Approaches
Non-parametric detectors forego explicit distributional assumptions and instead derive an empirical notion of “normality” from the data itself, typically via density estimation or neighbourhood comparison. {\bf Kernel Density Estimation (KDE)} constructs a smooth probability surface over the feature space; observations that fall in low-density regions of this surface are deemed anomalous.\cite[outlierSurvey]

Distance-based alternatives such as {\bf Local Outlier Factor (LOF)} contrast the local reachability density of each point with that of its neighbours, enabling the discovery of small clusters of contextual outliers that would be missed by global statistics.\cite[outlierSurvey]

Extensive empirical comparisons across dozens of benchmark datasets show that KDE, LOF and related graph-based methods achieve competitive accuracy and robustness to distributional misspecification, albeit at higher computational cost in high-dimensional settings. \cite[outlierSurvey]

\secc Multivariate Statistical Techniques
Multivariate techniques exploit the covariance structure among variables to measure joint extremeness and thus capture interactions that are invisible in any single dimension. The {\bf Mahalanobis distance} computes the squared distance from each observation to the multivariate mean, scaled by the inverse covariance matrix; under multivariate normality this statistic follows a $χ²$ distribution, allowing probabilistic outlier thresholds and robust extensions for contaminated data. \cite[outlierSurvey]

Dimensionality reduction methods such as {\bf Principal Component Analysis (PCA)} model the dominant low-rank subspace of the data and treat observations with large residual (reconstruction) error in the orthogonal complement as anomalies, a strategy that has proven effective for moderate-dimensional numeric datasets. \cite[outlierSurvey]

Comparative evaluations confirm that covariance- and subspace-based detectors remain competitive on structured numerical data but may struggle with heterogeneous categorical attributes, motivating hybrid or ensemble approaches in practical application. \cite[outlierSurvey]

\secc Overview
Statistical approaches treat normality as a probabilistic hypothesis and declare observations anomalous when they fall in the tails of the distribution. Parametric tests, for example, probability density estimation under a fitted Gaussian or Poisson model, work best for low-dimensional continuous variables whose shape is well captured by a single distribution; they offer simple likelihood thresholds but fail when the true data are multimodal or skewed. For purely categorical or discretised counts, the Pearson Chi-square test compares observed cell frequencies to expected ones and flags categories whose contribution to the $χ²$ statistic exceeds the critical value, making it a natural choice when expected counts are reliable and independence across cells is plausible.

Metrans transportation order data are tabular, text-heavy and possibly multimodal data. Parametric models will obvisously struggle as there is no underlying distribution. Non parametric models will not be able to handle the high-dimensional, sparse spaces produced by bag-of-words or TF-IDF encodings. And we do not have continuous linear dependent variables we would like in for multivariate statisticla models. Overall these techniques do not seem appropriate for our data.


\sec Clustering-Based Approaches to Anomaly Detection

Clustering-based anomaly detection methods group data points into clusters based on their similarity and identify anomalies as points that do not belong to any cluster or are significantly far from cluster centroids. These methods are unsupervised, making them suitable for scenarios where no labeled data is available. \cite[dataClustering]

Clustering-based anomaly detection relies on two fundamental principles. First, normal observations form dense, well-defined clusters, whereas anomalies manifest as isolated points or occupy regions of markedly low density. Second, by measuring the distance of each observation to its corresponding cluster centroid or to cluster boundaries, one can distinguish anomalous points as those whose distances exceed typical thresholds.\cite[dataClustering, Aggarwal2016]


\secc K-Means
The k-means algorithm partitions the dataset into a pre-specified number of clusters by iteratively assigning each point to the cluster with the nearest centroid and then recalculating centroids to minimize the within-cluster sum of squared distances. Once convergence is reached, points exhibiting unusually large distances from their assigned centroids are deemed anomalous. The appeal of k-means lies in its straightforward implementation and its computational efficiency, which render it suitable for large-scale datasets. However, its requirement to predefine the number of clusters can lead to suboptimal performance when the true cluster count is unknown, and its implicit assumption that clusters are spherical and of comparable size often fails for complex, real-world data distributions. \cite[Aggarwal2016]

\secc DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
Density-Based Spatial Clustering of Applications with Noise (DBSCAN) identifies clusters as contiguous regions of high point density, classifying observations in sparse regions as noise—that is, anomalies. This approach accommodates clusters of arbitrary shape and obviates the need to specify the number of clusters in advance. Nevertheless, its efficacy hinges on the appropriate selection of the neighborhood radius ($ε$) and the minimum number of points required to form a dense region ($minPts$). Moreover, DBSCAN may struggle when confronted with datasets exhibiting clusters of widely varying density. \cite[Aggarwal2016]

\secc Hierarchical Clustering
Hierarchical clustering constructs a dendrogram representing nested groupings of observations, either by successively merging the closest clusters in an agglomerative (bottom-up) fashion or by recursively dividing the dataset in a divisive (top-down) manner. Observations that remain isolated at a chosen level of the hierarchy, or that appear as singleton clusters, are flagged as outliers. This multi-scale perspective facilitates the detection of anomalies at varying levels of granularity. The principal drawback of hierarchical methods is their computational complexity, which grows rapidly with dataset size and can render them impractical for very large datasets.\cite[Aggarwal2016]



\secc Gaussian Mixture Models (GMM)
Gaussian Mixture Models (GMMs) posit that the data are generated by a linear combination of multiple Gaussian distributions. By fitting such a mixture using expectation-maximization, one obtains probabilistic descriptions of cluster membership. Observations that yield low likelihoods under all fitted Gaussian components are classified as anomalous. The probabilistic nature of GMMs allows one to derive interpretable anomaly scores based on estimated densities. However, the assumption that clusters follow Gaussian shapes limits their applicability in cases where the true data structure deviates substantially from normality. \cite[Aggarwal2016]

\secc Spectral Clustering
Spectral clustering represents the dataset as a similarity graph and employs the eigenvalues and eigenvectors of the graph Laplacian to project observations into a lower-dimensional space, where clusters become more evident. Points with weak connectivity in this spectral embedding are identified as anomalies. This technique excels at discovering complex, non-linear cluster geometries that challenge conventional distance-based methods. Its principal limitation is the computational burden associated with constructing and decomposing large affinity matrices, which can impede scalability on sizable datasets. \cite[Aggarwal2016]

\secc Strengths and Limitations
Clustering-based anomaly detection offers clear advantages: it does not require labeled data, adapts readily to diverse data types, and provides an intuitive framework in which anomalies are defined by deviations in distance or density. Nonetheless, these methods exhibit sensitivity to parameter choices—such as the number of clusters, density thresholds, and affinity definitions—and they frequently underperform in high-dimensional settings due to the curse of dimensionality. Furthermore, algorithms with hierarchical or spectral foundations often incur prohibitive computational costs when applied to large datasets. To mitigate these challenges, practitioners commonly combine clustering with feature engineering or dimensionality reduction techniques, thereby enhancing both detection accuracy and computational feasibility.\cite[Aggarwal2016]

\sec Isolation Forest and Tree-Based Methods for Anomaly Detection
Tree-based methods exploit the hierarchical structure of decision trees to isolate observations with unusual characteristics. These approaches are well suited for unsupervised anomaly detection because they do not require labeled data and can accommodate mixed data types. By recursively partitioning the feature space, tree-based techniques effectively separate rare data points from the bulk of the distribution, making them robust tools for identifying outliers in diverse datasets. \cite[Aggarwal2016]

\secc Isolation Forest
Isolation Forest (iForest, often abbreviated as iForest, was explicitly designed to detect anomalies by capitalizing on the fact that abnormal instances are typically few in number and exhibit distinct attribute values. The algorithm begins by constructing an ensemble of trees through random partitioning: at each node, it selects a feature at random and splits the data at a randomly chosen value within that feature’s observed range. This recursive splitting continues until every instance is isolated or a prescribed maximum tree depth is reached. The number of splits required to isolate an instance—the depth of the leaf in which it resides—is used to compute its anomaly score.\cite[isoForest]

Shorter paths correspond to higher anomaly scores since fewer splits imply easier isolation. Isolation Forest operates in linear time relative to the number of observations, scales effectively to large datasets, and naturally handles both categorical and numerical features. However, its performance may depend on the choice of hyperparameters such as the number of trees and the subsampling size. Moreover, the algorithm may encounter difficulties when anomalies closely resemble the majority class, potentially requiring additional feature engineering or complementary detection methods.\cite[isoForest]

\secc Decision Trees for Anomaly Detection
When labeled data are available, decision trees such as CART or C4.5 can be trained to distinguish between normal and anomalous examples in a supervised learning framework. In this context, the tree learns explicit decision rules that separate the two classes, although it may struggle to capture subtle or context-dependent anomalies without extensive feature design.\cite[Aggarwal2016]

In the absence of labels, decision tree logic can be adapted to identify majority-class patterns and to flag deviations as outliers. This unsupervised strategy typically involves growing trees with either random or density-guided splits and then defining thresholds on the resulting leaves: observations that fall into rarely populated or low-density leaves are flagged as anomalies. Despite their interpretability and ability to process mixed data types, standalone decision trees are prone to overfitting if not properly pruned or tuned, and they tend to be less robust in high-dimensional settings compared to ensemble methods.\cite[Aggarwal2016]

\secc Random Forests for Anomaly Detection
Although random forests are commonly employed for classification and regression tasks, they can be adapted for anomaly detection by leveraging the distribution of data points across the ensemble’s leaf nodes. For each tree in the forest, one records the leaf into which a data point falls; leaves that contain few observations are indicative of uncommon patterns.\cite[Aggarwal2016]

By aggregating these rarity measures across all trees, one derives an overall anomaly score for each instance. This ensemble approach benefits from the robustness and variance-reduction properties inherent to random forests and maintains compatibility with both categorical and numerical features. Nevertheless, it is more computationally intensive than simpler tree-based methods, and interpreting the resulting anomaly scores may require additional care to account for the ensemble’s complexity. \cite[Aggarwal2016]

\secc Overview of Tree-based methods
In summary, tree-based methods offer a compelling balance between interpretability and computational efficiency in anomaly detection. Isolation Forest provides a fast, scalable, and unsupervised baseline that excels in many scenarios, while decision trees and random forests extend this framework by introducing supervised classification capabilities or ensemble-based robustness. Selecting the appropriate method depends on the availability of labels, the dimensionality of the data, and the need for interpretability versus raw detection performance.\cite[Aggarwal2016]

\sec Deep-learning-based methods
Deep learning (DL) has rapidly enriched the anomaly detection toolbox by replacing handcrafted representations with features learned automatically from raw data, often yielding large accuracy gains in images, audio and high-dimensional sensor streams. Unlike classic statistical or tree-based methods that work directly in the original space, modern DL detectors embed inputs into a non-linear manifold where ``normality'' is easier to model and deviations stand out. \cite[deeplearninganomalydetection]

Deep learning methods offer several advantages for anomaly detection. They excel at capturing complex, non-linear relationships through learned representations, often outperforming traditional models on high-dimensional or unstructured data such as images, time series, or system logs. These models reduce the need for manual feature engineering and scale well with increasing data volume. Furthermore, they are well-suited for multimodal inputs, enabling integration of textual, visual, and numerical data into unified detectors. \cite[huang2025deeplearningadvancementsanomaly]

However, their limitations are significant. Deep models require large, clean datasets and are sensitive to contamination. Training and inference are computationally demanding, and their latency is typically higher than classical methods. Moreover, interpretability remains limited despite recent efforts using explainability tools, which poses challenges in regulated or operationally critical settings. On small to medium-sized tabular data, traditional methods like Isolation Forest or HBOS frequently outperform deep models in both accuracy and efficiency.  \cite[DL-worth-it]  Hyperparameter tuning and overfitting also remain practical concerns. \cite[huang2025deeplearningadvancementsanomaly]

Deep learning is most appropriate for image-based tasks, high-frequency multivariate time series, unstructured logs, and graph-structured data. In contrast, for structured, tabular datasets, such as those used in this thesis, classical models remain more suitable due to their lower complexity, higher interpretability, and comparable or superior performance.

\sec Summary

Statistical methods model expected patterns in the data and flag orders that deviate significantly. For categorical transport data, this often means identifying category values or combinations that are exceedingly rare or improbable under the historical distribution. For example, a {\em simple frequency-based approach} might assign anomaly scores to records containing infrequent sender-receiver combinations or unusual flag settings. These techniques are straightforward but mainly catch obvious outliers (e.g. a never-before-seen category value) and can miss anomalies that only emerge from subtle interactions between fields.

Clustering methods detect anomalies by grouping similar instances and isolating those that don’t fit well in any cluster. Algorithms like k-means (or its categorical counterpart k-modes) form clusters of typical orders, then label points in sparse regions or far from any cluster centroid as outliers. Likewise, density-based clustering (e.g. DBSCAN) can mark orders as anomalous “noise” if they have no dense neighbor region. Clustering considers multiple features jointly and can reveal unusual combinations; however, it requires defining a distance or similarity measure for categorical data and may struggle with very high-dimensional one-hot encoded inputs.

Tree-based methods such as Isolation Forest use ensembling of decision trees to separate anomalies from normal points. Isolation Forest randomly splits features; anomalous records (with unique or infrequent categories) tend to be isolated with fewer splits, yielding higher anomaly scores. These methods handle heterogeneous tabular data well – decision trees can branch on categorical features (especially with encodings or native support) to capture logical rules. 

Deep learning methods model the normal data distribution with neural networks to catch outliers as deviations. An autoencoder network, for instance, can be trained to reconstruct normal orders, then orders that reconstruct poorly (high error) are flagged as anomalies. Such models can capture complex feature dependencies that simpler methods might miss, improving detection accuracy when ample data and computational resources are available.


\nocite[aima]