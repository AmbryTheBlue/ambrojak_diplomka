% !TEX program = optex
% !TEX root = ambrojak-thesis-masters.tex
\label[methods]
\chap Methods for Anomaly Detection
\rfc{This chapter (Theorethical background and approaches) should probably be revised. Maybe moved after anomaly and stated which models to use based on our data or keep it here and more thereical/general? But I do not like the writing style very much.}


\sec Introduction to Anomaly Detection
Anomaly detection (sometimes shortened as \glref{AD}), also referred to as outlier detection (sometimes abbreviated as \glref{OD}), is the task of identifying instances in a dataset that deviate significantly from the majority of the data. These deviations, termed anomalies, often represent rare or unusual events, errors, or significant phenomena of interest. Applications of anomaly detection span diverse fields, including fraud detection, system monitoring, medical diagnostics, and natural language processing. \cite[chandola2009]

\secc Supervised Anomaly Detection
In supervised anomaly detection, models are trained on fully labeled datasets that contain examples of normal and anomalous. A classifier learns to distinguish between the two classes and flags deviations accordingly.
Although this approach can achieve high accuracy for known anomaly types, it requires extensive labeled data, which is often impractical, because anomalies are, by definition, rare and expensive to label. \cite[ibmAnomalyDetection]

\secc Semi-Supervised Anomaly Detection
Semi-supervised methods leverage a small quantity of labeled data (typically only normal instances) alongside a large pool of unlabeled data to build a model of normal behavior and detect deviations.
These techniques are particularly useful when anomalous labels are scarce but normal examples are abundant. By modeling only the “normal” class, they flag instances that do not fit this pattern as anomalies.\cite[ibmAnomalyDetection]


\secc Unsupervised Anomaly Detection
Unsupervised anomaly detection is particularly relevant in cases where labeled data (that is, explicit examples of anomalous and normal instances) are unavailable. In this paradigm, the algorithm assumes that anomalies are rare and distinct from the ``normal'' data. The approach involves modeling the patterns, distributions, or structure of normal data and identifying data points that deviate significantly from these learned patterns.\cite[unsupervisedAnomalyDetection]

This type of problem describes scenarios where only examples of normal behavior are available. It is necessary to use techniques such as clustering, reconstruction errors, distance measures, or density estimation; unsupervised anomaly detection provides a robust framework for identifying unexpected patterns without relying on prior knowledge of what constitutes an anomaly.\cite[unsupervisedAnomalyDetection]

\secc Summary
In summary, the choice among supervised, semi-supervised, and unsupervised anomaly detection depends primarily on the availability of labeled examples of anomalous behavior and the expected complexity of normal data distributions.

In the next section, we will survey specific algorithmic families suited to different data characteristics, before selecting those most appropriate for the Metrans transport order dataset.

\sec Statistical Approaches to Anomaly Detection

Statistical methods for anomaly detection are based on the assumption that the data follow a specific distribution or statistical model. Anomalies are identified as points that deviate significantly from this assumed distribution or exhibit rare or improbable characteristics. These methods are particularly effective when the underlying data structure is well understood and the anomalies are clearly distinguishable from normal variation.\cite[chandola2009]

\secc Parametric Approaches
Parametric anomaly-detection techniques posit that normal data are drawn from a member of a known distributional family (most commonly Gaussian, Poisson, or exponential) and then quantify how improbable each observation is under maximum-likelihood parameter estimates. A prototypical example is the {\bf Z-score test}, which flags points whose standardized distance from the sample mean exceeds a user-defined threshold (e.g., |z| > 3) when normality holds. Formal single-point hypothesis tests such as the {\bf Grubbs test} or the generalized extreme studentized deviation extend this idea to small samples and heavy-tailed distributions by comparing each candidate point to critical values of the assumed model. Although these methods are computationally simple and interpretable, large benchmark studies demonstrate that their accuracy drops sharply when the true data-generating process is multimodal or skewed, highlighting their sensitivity to model misspecification \cite[outlierSurvey, chandola2009]

Anomaly detection based on {\bf PDE} ({\bf Probability Density Estimation})first fits a parametric distribution to the 'normal' data, most commonly by maximum likelihood estimation of a Gaussian, Poisson, or (in low-dimensional mixtures) a finite-mixture model. Each observation $x_i$ is then evaluated under the fitted density $p_θ(x)$; points whose log-likelihood falls below a significance threshold (or equivalently whose tail probability is smaller than $\alpha$ are flagged as anomalies. Because the entire decision rule is derived from the assumed form of $p_θ$. PDE yields interpretable probabilistic scores and tight control over the false-alarm rate when the model specification is valid. Its chief weakness is model misspecification: multimodal, skewed, or heavy-tailed real-world data can cause likelihoods to be badly calibrated, leading to false positives and false negatives, a phenomenon extensively documented in comparative surveys of outlier detection algorithms. \cite[chandola2009]


The {\bf Chi-Square test} is a non-parametric statistical method used to determine whether there is a significant association between two categorical variables. It compares the observed frequencies in each category of a contingency table with the frequencies expected under the assumption of independence. The test statistic is calculated as:
$$
\chi^2 = \sum {(O-E)^2 \over E}
$$
where $O$ represents the observed frequency and $E$ denotes the expected frequency under the null hypothesis of independence. A large Chi-Square statistic indicates a significant difference between observed and expected frequencies, suggesting a potential association between the variables.\cite[chi-square]

In the context of anomaly detection, the Chi-Square test can be applied to identify unusual patterns in categorical data. For instance, it can detect deviations in user behavior on a website by comparing current page visit frequencies to historical norms. Significant differences may indicate anomalies such as fraudulent activities or system malfunctions.\cite[chi-square]

However, the Chi-Square test has limitations. It requires a sufficient sample size to ensure the validity of the approximation to the Chi-Square distribution. Specifically, expected frequencies in each cell of the contingency table should generally be at least 5. When this condition is not met, the test's reliability diminishes, and alternative methods, such as Fisher's Exact Test, may be more appropriate.\cite[chi-square]


\secc Non-Parametric Approaches
Non-parametric detectors forego explicit distributional assumptions and instead derive an empirical notion of “normality” from the data itself, typically via density estimation or neighbourhood comparison. {\bf Kernel Density Estimation (KDE)} constructs a smooth probability surface over the feature space; observations that fall in low-density regions of this surface are deemed anomalous.\cite[outlierSurvey]

Distance-based alternatives such as {\bf Local Outlier Factor (LOF)} contrast the local reachability density of each point with that of its neighbours, enabling the discovery of small clusters of contextual outliers that would be missed by global statistics.\cite[outlierSurvey]

Extensive empirical comparisons across dozens of benchmark datasets show that KDE, LOF and related graph-based methods achieve competitive accuracy and robustness to distributional misspecification, albeit at higher computational cost in high-dimensional settings. \cite[outlierSurvey]

\secc Multivariate Statistical Techniques
Multivariate techniques exploit the covariance structure among variables to measure joint extremeness and thus capture interactions that are invisible in any single dimension. The {\bf Mahalanobis distance} computes the squared distance from each observation to the multivariate mean, scaled by the inverse covariance matrix; under multivariate normality this statistic follows a $χ²$ distribution, allowing probabilistic outlier thresholds and robust extensions for contaminated data. \cite[outlierSurvey]

Dimensionality reduction methods such as {\bf Principal Component Analysis (PCA)} model the dominant low-rank subspace of the data and treat observations with large residual (reconstruction) error in the orthogonal complement as anomalies, a strategy that has proven effective for moderate-dimensional numeric datasets. \cite[outlierSurvey]

Comparative evaluations confirm that covariance- and subspace-based detectors remain competitive on structured numerical data but may struggle with heterogeneous categorical attributes, motivating hybrid or ensemble approaches in practical application. \cite[outlierSurvey]

\secc Overview
Statistical approaches treat normality as a probabilistic hypothesis and declare observations anomalous when they fall in the tails of the distribution. Parametric tests, for example, probability density estimation under a fitted Gaussian or Poisson model, work best for low-dimensional continuous variables whose shape is well captured by a single distribution; they offer simple likelihood thresholds but fail when the true data are multimodal or skewed. For purely categorical or discretised counts, the Pearson Chi-square test compares observed cell frequencies to expected ones and flags categories whose contribution to the $χ²$ statistic exceeds the critical value, making it a natural choice when expected counts are reliable and independence across cells is plausible.

Metrans transportation order data are tabular, text-heavy and possibly multimodal data. Parametric models will obvisously struggle as there is no underlying distribution. Non parametric models will not be able to handle the high-dimensional, sparse spaces produced by bag-of-words or TF-IDF encodings. And we do not have continuous linear dependent variables we would like in for multivariate statisticla models. Overall these techniques do not seem appropriate for our data.


\sec Clustering-Based Approaches to Anomaly Detection

Clustering-based anomaly detection methods group data points into clusters based on their similarity and identify anomalies as points that do not belong to any cluster or are significantly far from cluster centroids. These methods are unsupervised, making them suitable for scenarios where no labeled data is available. \cite[dataClustering]

\secc Core Principles
Cluster Membership:
\begitems
* Normal data points form dense, well-defined clusters.
* Anomalies are isolated points or lie in low-density regions.
\enditems
Distance to Clusters:
\begitems
* Anomalies are distant from cluster centroids or boundaries.
\enditems


\secc K-Means
Methodology:
\begitems
* Assigns data points to k clusters by minimizing the distance to the nearest cluster centroid.
* Anomalies are points with high distances from their assigned cluster centroid.
\enditems
\nl
Strengths:
\begitems
* Simple and computationally efficient.
\enditems
\nl
Limitations:
\begitems
* Requires predefining the number of clusters (k).
* Assumes clusters are spherical and equally sized, which may not align with real-world data.
\enditems
\secc DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
Methodology:
\begitems
* Groups points into clusters based on local density. Points in sparse regions are classified as noise (anomalies).
\enditems
\nl
Strengths:
\begitems
* Handles arbitrary cluster shapes.
* Does not require specifying the number of clusters.
\enditems
\nl
Limitations:
\begitems
* Sensitive to the choice of parameters (e.g., neighborhood radius ε and minimum points).
* Struggles with varying density levels.
\enditems

\secc Hierarchical Clustering
Methodology:
\begitems
* Constructs a hierarchy of clusters via agglomerative (bottom-up) or divisive (top-down) methods.
* Anomalies are points that remain unclustered at a specific level of the hierarchy or form singleton clusters.
\enditems
\nl
Strengths:
\begitems
* Provides a flexible structure for identifying outliers at various levels.
\enditems
\nl
Limitations:
\begitems
* Computationally expensive for large datasets.
\enditems

\secc Gaussian Mixture Models (GMM)
Methodology:
\begitems
 * Assumes data is generated from a mixture of Gaussian distributions.
* Anomalies are points with low probabilities under the fitted Gaussian components.
\enditems
\nl
Strengths:
\begitems
* Probabilistic foundation provides interpretable scores.
\enditems
\nl
Limitations:
\begitems
* Assumes Gaussian clusters, which may not suit all data.
\enditems

\secc Spectral Clustering
Methodology:
\begitems
* Constructs a similarity graph and uses its eigenvalues to form clusters.
* Points with weak connections to any cluster are flagged as anomalies.
\enditems
Strengths:
\begitems
* Effective for non-linear cluster structures.
\enditems
Limitations:
\begitems
* Computationally intensive for large datasets.
\enditems

\secc Strengths and Limitations
{\em Strengths:}
\begitems
* {\bf Unsupervised:} No need for labeled data.
* {\bf Versatile:} Handles various types of data.
* {\bf Intuitive:} Anomalies are naturally defined by distance or density.
\enditems
{\em Limitations:}
\begitems
* {\bf Parameter Sensitivity:} Performance depends on parameters like k, ε, or minimum density.
* {\bf High Dimensionality:} Clustering algorithms often struggle in high-dimensional spaces ("curse of dimensionality").
* {\bf Scalability:} Methods like hierarchical or spectral clustering are computationally expensive for large datasets.
\enditems

Clustering-based methods are widely used in anomaly detection and are often combined with feature engineering or dimensionality reduction to improve performance.

\sec Isolation Forest and Tree-Based Methods for Anomaly Detection

Tree-based methods leverage the structure of decision trees to isolate anomalies based on their distinct characteristics. These techniques are particularly suited for unsupervised anomaly detection, as they do not rely on labeled data and work well with mixed data types.

\secc Isolation Forest
Isolation Forest ({\em iForest}) is explicitly designed for anomaly detection. It operates on the principle that anomalies are easier to isolate than normal points because they are rare and distinct. \cite[isoForest]
\nl
{\em Description of the core algorithm}
\begitems
* {\bf Random Partitioning:} Randomly select a feature and split the dataset at a randomly chosen value within the feature's range.
* {\bf Recursive Splitting:} Build a tree by recursively partitioning the data until all points are isolated or a maximum tree depth is reached.
* {\bf Anomaly Scoring:} The depth of a point in the tree (number of splits required to isolate it) is inversely proportional to its anomaly score. Points isolated in fewer splits (shorter depth) are considered anomalous.
\enditems
\nl
Strengths:
\begitems
* {\bf Computational Efficiency:} Works in linear time with respect to the number of data points.
* {\bf Handles Mixed Data:} Suitable for both categorical and numerical features.
* {\bf Scalable:} Works well with large datasets.
\enditems
\nl
Limitations
\begitems
* Requires parameter tuning (e.g., number of trees, subsampling size).
* May struggle with data where anomalies closely resemble the majority class.
\enditems

\secc Decision Trees for Anomaly Detection
{\bf Supervised Context: }
When labeled data is available, decision trees (e.g., CART, C4.5) can classify anomalies explicitly. Fit a tree to distinguish between "normal" and "anomalous" data. Less interpretable in the presence of subtle anomalies.


{\bf Unsupervised Context: }
Adapt decision tree logic to identify patterns in the majority class and flag outliers. Use thresholds for decision paths where certain leaf nodes represent unusual patterns. Random splits or density-based measures are employed to enhance detection.
\nl
Strengths
\begitems
* Easy to interpret, especially for small trees.
* Handles both numerical and categorical data.
\enditems
Limitations
\begitems
* Can overfit without proper pruning or parameter tuning.
* Less robust to high-dimensional data compared to ensemble methods like Isolation Forest.
\enditems

\secc Random Forests for Anomaly Detection
Random forests, while typically used for classification or regression, can be adapted for anomaly detection:
\begitems
* Compute leaf nodes for each tree and measure how often a data point lands in rare or low-density leaves.
* Combine these measures to produce an anomaly score.
\enditems
\nl
Strengths:
\begitems
* Benefits from ensemble robustness.
* Can handle diverse types of data.
\enditems
\nl
Limitations
\begitems
* Computationally more expensive than simpler tree-based models.
* Requires careful interpretation of output anomaly scores
\enditems

\sec Deep-learning-based methods
Deep learning (DL) has rapidly enriched the anomaly detection toolbox by replacing handcrafted representations with features learned automatically from raw data, often yielding large accuracy gains in images, audio and high-dimensional sensor streams. Unlike classic statistical or tree-based methods that work directly in the original space, modern DL detectors embed inputs into a non-linear manifold where ``normality'' is easier to model and deviations stand out. \cite[deeplearninganomalydetection]

Deep learning methods offer several advantages for anomaly detection. They excel at capturing complex, non-linear relationships through learned representations, often outperforming traditional models on high-dimensional or unstructured data such as images, time series, or system logs. These models reduce the need for manual feature engineering and scale well with increasing data volume. Furthermore, they are well-suited for multimodal inputs, enabling integration of textual, visual, and numerical data into unified detectors. \cite[huang2025deeplearningadvancementsanomaly]

However, their limitations are significant. Deep models require large, clean datasets and are sensitive to contamination. Training and inference are computationally demanding, and their latency is typically higher than classical methods. Moreover, interpretability remains limited despite recent efforts using explainability tools, which poses challenges in regulated or operationally critical settings. On small to medium-sized tabular data, traditional methods like Isolation Forest or HBOS frequently outperform deep models in both accuracy and efficiency.  \cite[DL-worth-it]  Hyperparameter tuning and overfitting also remain practical concerns. \cite[huang2025deeplearningadvancementsanomaly]

Deep learning is most appropriate for image-based tasks, high-frequency multivariate time series, unstructured logs, and graph-structured data. In contrast, for structured, tabular datasets, such as those used in this thesis, classical models remain more suitable due to their lower complexity, higher interpretability, and comparable or superior performance.

\sec Overview 
Tree-based methods are particularly valuable when interpretability and computational efficiency are critical. Isolation Forest, in particular, is a strong baseline for anomaly detection in many scenarios.
\nl
\cite[aima, outlierSurvey]