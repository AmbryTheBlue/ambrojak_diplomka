% !TEX program = optex
% !TEX root = ambrojak-thesis-masters.tex
\label[results]
\chap Evaluation of Detection Models

\sec Evaluation methods
Modern anomaly detection systems borrow their performance metrics from binary classification theory: they compare the predictions of the model (or anomaly scores) with ground-truth labels and visualize the trade-offs that arise when a score threshold is moved. Below is a concise overview of the core tabular metrics—confusion matrix, precision, recall and F1—and the threshold-sweeping visualisations.

\secc Confusion Matrix
The confusion matrix is a 2×2 contingency table that enumerates these four outcomes and forms the basis for almost every downstream metric.\cite[mediumConfusionMatrix]

\midinsert \clabel[confusionMatrixTable]{Confusion Matrix}
\ctable{l|cc}{\hfil
Actual/Prediction  & Predicted Positive & Predictive Negative \crli \tskip4pt
Actual Positive & True Positive (TP) & False Negative (FN) \cr
Actual Negative & False Positive (FP) & True Negative (TN) \cr
}
\caption/t Table explaining Confusion Matrix
\endinsert
{\bf In this thesis we always view the anomalous class as positive.} The negative is therefore referring to normal (nonanomalous) data. In other words, we are asking the question ``Is this datum anomalous?''

The table \ref[confusionMatrixTable] is explaining TP, FP, FN and TN. And an example of how to use them to compare several models can be seen in Figure \ref[confusionMatrixFigure]

\medskip
\clabel[confusionMatrixFigure]{Confusion Matrix for models}
\picw=15cm \cinspic imgs/ConfusionMatrix.pdf
\caption/f Confusion matrix for FBOD models (on Customs Anomaly)
\medskip

Also there is a need to mention Type I and Type II errors. Type I error is an alternative name for FP (False Positive). And Type II error is a synonym for FN (False Negative) \cite[Chadha2020DistilledEvaluationMetrics]. They are, in my opinion, unnecessarily confusing names. And there is no need to mention them anywhere else in this thesis, as the terms FP and FN are sufficient, and furthermore, their names already hint at their meaning.

\secc Accuracy
The simplest way to measure the performance of the model is to measure the frequency with which the prediction of the model matches the actual class. This measure may hide crucial information when one class is very rare \cite[Chadha2020DistilledEvaluationMetrics].

This is the case with anomalous data that were provided by Metrans. Although this can be mitigated by synthetic anomaly generation that can even out the distribution of the classes and make this measure once again somewhat useful.

$$\rm
{Accuracy} = {TP + TN \over TP + TN + FP + FN} \eqmark[accuracy]
$$

\label[precision]
\secc Precision
Precision (Positive Predictive Value, PPV) quantifies the purity of the detected anomalies \cite[mediumConfusionMatrix]. It answers the question ``When detector predicts that the datum is positive (anomalous), how often is it right?''

$$\rm
{Precision} = {TP \over TP + FP} \eqmark[eq-precision]
$$

% $$
% {Precision} = {TP \over TP + FP}
% $$
% $$\it
% {Precision} = {TP \over TP + FP}
% $$
% $$\cal
% {PRECISIONrecision} = {TP \over TP + FP}
% $$
% $$\mit
% {Precision} = {TP \over TP + FP}
% $$
% $$\bf
% {Precision} = {TP \over TP + FP}
% $$

\label[recall]
\secc Recall
Recall (Sensitivity, True Positive Rate, TPR) measures completeness\cite[mediumConfusionMatrix]. It answers ``How many real anomalies did we catch?''
$$\rm
{Recall} = {TP \over TP + FN} \eqmark[eq-recall]
$$

\label[specificity]
\secc Specificity
Specificity (True Negative Rate, TNR) measures the proportion of actual negatives correctly identified \cite[Chadha2020DistilledEvaluationMetrics]. It answers the question ``How good are we at detecting OK data?''.

This metric is not very useful for our case. We care about detecting anomalies, as not detecting them is costly. Detecting nonanomalous data is not crucial. This metric is usefull for minimizing false alarms. But in our case these are not very costly.
$$\rm
{Specificity} = {TN \over  TN + FP} \eqmark[eq-specificity]
$$

\label[npv]
\secc Negative Predictive Value (NPV):
Negative Predictive Value (NPV) represents the probability that instances predicted as normal are truly normal \cite[Chadha2020DistilledEvaluationMetrics]. It answers the question ``If we predict that datum is OK, how likely is it actually OK?''.

This metric is again not very useful for our case. A high NPV suggests confidence in the model's negative predictions, ensuring that normal instances are not misclassified as anomalies. And this once again is not that costly of a mistake in our case.
$$\rm
{NPV} = {TN \over TN + FN} \eqmark[npv]
$$

\secc F1 Score
F1 score is the harmonic mean Precision \ref[precision] and Recall \ref[recall]. It gives a single number that penalizes detectors that are skewed toward either high precision or toward high recall \cite[mediumConfusionMatrix].

This metric is very usefull in our case as it focuses on detecting anomalies. But it also gives a singular number that allows us to evaluate our models and rank them from best to worst. For an example graph for comparison of several models using F1 (and Precision and Recall), see Figure \ref[FigureF1].

$$\rm
{F1} = 2 \times { Precision \times Recall \over Precision + Recall} \eqmark[eq-f1]
$$

\medskip
\clabel[FigureF1]{Typical metrics for model comparsion}
\picw=15cm \cinspic imgs/Metrics.pdf
\caption/f Typical metrics for anomaly detection used for model comparison
\medskip

\sec Visualization Techniques
We have also tried several visualization techniques to provide better insight into the results. These techniques use Anomaly scores, that is (for most models) a probability score of the datum being an anomaly.

\secc Anomaly Score Distribution

An anomaly‐score distribution graph displays how the computed scores for each data point—whether normal or anomalous—are spread across the range of possible values. Typically, you’ll see two overlapping curves or histograms: one representing the bulk of “normal” orders clustered at lower scores, and a sparser tail or separate peak at higher scores corresponding to the few truly anomalous entries. By visualizing these distributions side by side, you can choose a threshold score that best separates the two populations, minimizing the chance of misclassifying a normal order as anomalous while still catching the majority of true anomalies. The degree of overlap between the curves indicates how difficult the detection problem is: the less overlap, the more confidently you can distinguish outliers from routine orders.

\medskip
\clabel[AnomalyScore]{Anomaly Score Distribtion}
\picw=15cm \cinspic imgs/AnomalyScores.pdf
\caption/f Distrbution of anomaly scrores by Voting Classifier Heating Type Anomaly (appears to be easily separable)
\medskip

\label[ROC]
\secc Reciever Operating Characteristic
A {\bf Receiver Operating Characteristic (ROC)} curve traces how a classifier’s true positive rate changes as its false positive rate increases when you slide the decision threshold from strict to permissive. At each threshold, the true positive rate (also called sensitivity) measures the proportion of actual positives correctly identified, while the false positive rate measures the proportion of actual negatives that are incorrectly labeled as positives. Plotting the false positive rate on the x-axis against the true positive rate on the y-axis yields a curve that starts at the origin (no positives predicted) and ends at the point (1, 1) (all instances predicted positive). A ROC curve that bows sharply toward the top-left corner indicates that the model achieves high sensitivity while keeping the false alarm rate low across a broad range of thresholds; by contrast, a curve that clings close to the diagonal line reflects a model whose predictions are barely better than random guessing.\cite[murphy2012machine]

\medskip
\clabel[ROCCurve]{ROC Curve}
\picw=10cm \cinspic imgs/ROCcurve.pdf
\caption/f Example of Reciever Operating Characteristic Curve for Voting Classifier (Customs Anomaly)
\medskip

The {\bf Area Under the ROC curve} (often abbreviated as {\bf AUC-ROC}) provides a single-number summary of classification performance by quantifying the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative one. An AUC of 1.0 represents a perfect classifier that assigns higher scores to all positives than to all negatives, while an AUC of 0.5 corresponds to performance no better than chance. Because the ROC AUC considers both types of errors equally, it remains largely insensitive to changes in class balance; however, in scenarios where false positives carry a heavier cost than false negatives (or vice versa), one may still need to select an operating threshold that reflects the desired trade-off, using the ROC curve to identify the point where the increase in true positive rate no longer justifies the rise in false positives.\cite[murphy2012machine]

\label[PRC]
\secc Precision Recall Curve
A precision-recall (PR Curve, PRC) curve visualizes the trade-off between precision (section \ref[precision]) and recall (section \ref[recall]) for different decision thresholds of a probabilistic classifier. An example can be seen in figure \ref[PRCurve]. And the process of constructing in is outlined below
\begitems
* Compute scores. The classifier assigns each instance a score or probability of being positive (e.g., anomaly score, probability of ``anomalous'' class).

* Vary the threshold. For each threshold $t$ in $[0,1]$. Label as “positive” all instances with score $≥t$, others as “negative.”

* Compute the precision and recall

* Plot On the x-axis: recall; on the y-axis: precision. Connecting the points as the threshold moves from high (very strict) to low (very lenient) yields the PR curve.
\enditems

\medskip
\clabel[PRCurve]{Precision Recall Curve}
\picw=10cm \cinspic imgs/Precision-Recall.pdf
\caption/f Example of precision recall curve for Voting Classifier (Customs Anomaly)
\medskip

When examining a precision–recall curve, the ideal situation is represented by a point at the top-right corner, where both precision and recall equal 1, meaning every positive instance is found and there are no false positives. If the curve sits toward the upper-left region, it indicates that the classifier is highly precise but has low recall—it only labels as positive those instances it is extremely confident about, and consequently misses many true positives. Conversely, when the curve bulges toward the lower-right region, it reflects high recall but low precision, implying that although most actual positives are flagged, a large number of negative instances are incorrectly classified as positive. The overall shape of the curve matters: a curve that hugs the top-right corner demonstrates strong performance, whereas a sharp drop in precision for only a slight gain in recall suggests that lowering the threshold adds mostly false positives without substantially improving detection of true positives.\cite[murphy2012machine]

The {\bf Area Under the Precision–Recall Curve (AUPRC)} provides a single-number summary of this trade-off across all thresholds. Because it focuses exclusively on the behavior of the classifier with respect to the positive class, AUPRC is especially informative when positive examples are rare. In contrast to the ROC AUC, which averages performance in both classes and can appear deceptively high if the negative class dominates, the AUPRC more accurately reflects a model’s ability to find positive cases while avoiding false alarms in imbalanced settings. \cite[murphy2012machine]

\sec Results
\secc Heating Type
The performance of various models can be seen in \ref[tableHeatingTypeAnomalyBoW]. The best models reach F1 scores over $0.99$. In this table we have used our preprocessing methods, applied BoW vecotorization and used information about client, reciever, goods and chassis type.

In several more tables we have tried to decompose the major sources of our success. First of all we have removed our preprocessing (see Table \ref[tableHeatingTypeAnomalyNoPreproces]). Preprocessing seems to have made some improvement, but not much. I suspect that it is because internally BoW or TF-IDF provided by sklearn does the most crucial parts of preprocessing by itself (e.g. case unification). But it seem to have some small improvements for. At least FBOD models, see Table \ref[tableComparepreprocessingFBOD].

Second we have tried swichting from BoW to TF-IDF (see Table \ref[tableHeatingTypeAnomalyGoodsChassis]). Once again the TF-IDF version is a bit worse but not a significant ammount. It seem the best improvement over the baseline model \ref[tableHeatingTypeAnomaly] was adding more information about the order. Both chassis and goods information has helped, although goods seem to be the more important (see Table \ref[tableHeatingTypeAnomalyGoods]).

Overall even the performance of baseline models was very good. That is not suprising as further look into anomaly scores (see Figure \ref[AnomalyScore]) shows us that for most models the difference between anomalous and nonanomlaous data is significant. Despite that we have made some further Accuracy (and F1 score) gains using better preprocessing, vectorization and most importantly adding more info about the transportation order.
The models seem to have been Random Forests (as were expected) but are closely followed by CBAD with even unexpect estimators such as LogRegression or SGD.

\midinsert \clabel[tableHeatingTypeAnomalyBoW]{Model Comparison Heating Type Anomaly BoW}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
MD:rf & 0.498 & 0.008 & 0.492 & 0.002 & 0.990 & 0.996 & 0.984 & 0.990 \cr
CBAD:rf & 0.498 & 0.008 & 0.492 & 0.002 & 0.990 & 0.996 & 0.983 & 0.990 \cr
CBAD:sgdModifiedHuber & 0.497 & 0.009 & 0.491 & 0.003 & 0.989 & 0.995 & 0.983 & 0.989 \cr
CBAD:LogRegression & 0.498 & 0.009 & 0.491 & 0.002 & 0.989 & 0.995 & 0.983 & 0.989 \cr
MD:KNeighborsClassifier & 0.497 & 0.009 & 0.491 & 0.003 & 0.988 & 0.994 & 0.982 & 0.988 \cr
MD:KerasNNClassifier & 0.495 & 0.009 & 0.491 & 0.005 & 0.986 & 0.990 & 0.982 & 0.986 \cr
MD:VotingClassifier & 0.494 & 0.008 & 0.492 & 0.006 & 0.986 & 0.988 & 0.984 & 0.986 \cr
MD:lgbm & 0.494 & 0.008 & 0.492 & 0.006 & 0.985 & 0.988 & 0.983 & 0.985 \cr
MD:xgb & 0.494 & 0.008 & 0.492 & 0.006 & 0.985 & 0.988 & 0.983 & 0.985 \cr
FBODBackOff & 0.488 & 0.003 & 0.497 & 0.012 & 0.986 & 0.977 & 0.995 & 0.985 \cr
MD:LogRegression & 0.494 & 0.009 & 0.491 & 0.006 & 0.985 & 0.988 & 0.983 & 0.985 \cr
CBAD:sgdLogLoss & 0.494 & 0.009 & 0.491 & 0.006 & 0.985 & 0.988 & 0.982 & 0.985 \cr
CBAD:KNeighborsClassifier & 0.494 & 0.009 & 0.491 & 0.006 & 0.984 & 0.988 & 0.981 & 0.985 \cr
FBOD & 0.489 & 0.006 & 0.494 & 0.011 & 0.984 & 0.979 & 0.989 & 0.984 \cr
CBAD:VotingClassifier & 0.494 & 0.011 & 0.489 & 0.006 & 0.982 & 0.987 & 0.977 & 0.982 \cr
CBAD:ComplementNB & 0.491 & 0.020 & 0.480 & 0.009 & 0.970 & 0.981 & 0.960 & 0.971 \cr
CBAD:MultinomialNB & 0.490 & 0.020 & 0.480 & 0.010 & 0.969 & 0.979 & 0.960 & 0.970 \cr
CBAD:BernoulliNB & 0.486 & 0.028 & 0.472 & 0.014 & 0.959 & 0.973 & 0.946 & 0.959 \cr
}
\caption/t Performance metrics per model in Heating Type Anomaly with information about goods and chassis type using BoW (sorted by F1)
\endinsert

\midinsert \clabel[tableComparepreprocessingFBOD]{Model Compare preprocessing FBOD}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
FBODBackOff-preprocess & 0.489 & 0.003 & 0.497 & 0.011 & 0.986 & 0.978 & 0.994 & 0.986 \cr
FBODBackOff & 0.488 & 0.003 & 0.497 & 0.012 & 0.986 & 0.977 & 0.995 & 0.985 \cr
FBOD-preprocess & 0.490 & 0.005 & 0.495 & 0.010 & 0.985 & 0.980 & 0.990 & 0.985 \cr
FBOD & 0.489 & 0.006 & 0.494 & 0.011 & 0.984 & 0.979 & 0.989 & 0.984 \cr
}
\caption/t Performance metrics in FBOD with and without preprocessing (sorted by F1)
\endinsert

\secc Has Heating
We have tried several models, see table \ref[tableHasHeating]. The best models are reaching a point with units of errors per 10,000 orders. This makes sense as there are likely only a few companies in Central Europe that order tank heating. But this proves our decision to split of this anomaly from Heating type anomaly as important. As this anomaly is easier and would artificially improve our performance on the latter.
We do not try to adjust these models as this is already almost perfect performance.


\secc Customs number
We have tried several models to handle this anomaly, as can be seen in Table \ref[tableCustomsNumber]. Further improvements were made by adding more information about port destinations; see Table \ref[tableCustomsNumberPlace].

\midinsert \clabel[tableCustomsNumberPlace]{Model Comparison Customs number with place}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
MD:KerasNNClassifier & 0.496 & 0.004 & 0.496 & 0.004 & 0.992 & 0.992 & 0.992 & 0.992 \cr
MD:rf & 0.495 & 0.005 & 0.495 & 0.005 & 0.991 & 0.991 & 0.991 & 0.991 \cr
CBAD:rf & 0.495 & 0.005 & 0.495 & 0.005 & 0.991 & 0.991 & 0.991 & 0.991 \cr
MD:VotingClassifier & 0.495 & 0.005 & 0.495 & 0.005 & 0.991 & 0.991 & 0.991 & 0.991 \cr
CBAD:sgdModifiedHuber & 0.494 & 0.006 & 0.494 & 0.006 & 0.988 & 0.988 & 0.988 & 0.988 \cr
MD:LogRegression & 0.494 & 0.006 & 0.494 & 0.006 & 0.988 & 0.988 & 0.988 & 0.988 \cr
CBAD:LogRegression & 0.494 & 0.006 & 0.494 & 0.006 & 0.988 & 0.988 & 0.988 & 0.988 \cr
CBAD:VotingClassifier & 0.494 & 0.006 & 0.494 & 0.006 & 0.987 & 0.987 & 0.987 & 0.987 \cr
MD:KNeighborsClassifier & 0.494 & 0.006 & 0.494 & 0.006 & 0.987 & 0.987 & 0.987 & 0.987 \cr
CBAD:KNeighborsClassifier & 0.494 & 0.006 & 0.494 & 0.006 & 0.987 & 0.987 & 0.987 & 0.987 \cr
MD:lgbm & 0.493 & 0.007 & 0.493 & 0.007 & 0.986 & 0.986 & 0.986 & 0.986 \cr
MD:xgb & 0.492 & 0.008 & 0.492 & 0.008 & 0.985 & 0.985 & 0.985 & 0.985 \cr
FBODBackOff-preprocess & 0.486 & 0.002 & 0.498 & 0.014 & 0.984 & 0.973 & 0.996 & 0.984 \cr
FBODBackOff & 0.486 & 0.002 & 0.498 & 0.014 & 0.984 & 0.972 & 0.996 & 0.984 \cr
CBAD:sgdLogLoss & 0.492 & 0.008 & 0.492 & 0.008 & 0.984 & 0.984 & 0.984 & 0.984 \cr
FBOD-preprocess & 0.495 & 0.017 & 0.483 & 0.005 & 0.978 & 0.989 & 0.967 & 0.978 \cr
FBOD & 0.495 & 0.019 & 0.481 & 0.005 & 0.976 & 0.991 & 0.962 & 0.976 \cr
CBAD:MultinomialNB & 0.484 & 0.016 & 0.484 & 0.016 & 0.968 & 0.968 & 0.968 & 0.968 \cr
CBAD:BernoulliNB & 0.483 & 0.017 & 0.483 & 0.017 & 0.966 & 0.966 & 0.966 & 0.966 \cr
CBAD:ComplementNB & 0.481 & 0.019 & 0.481 & 0.019 & 0.961 & 0.961 & 0.961 & 0.961 \cr
}
\caption/t Performance metrics per model in Customs number with place (sorted by F1)
\endinsert

\secc Additional Billing Reference
This anomaly has proved trivial. Most reasonable models reached accuracy 99.95\% , so in table \ref[tableAdditionalReferenceExport] it appears as 100\% despite no model reaching that literally. This visualization uses only information about the client, adding also information about the receiver moves the accuracy past 99.97\%, even thought these improvements are rounded up we provide the Table \ref[tableAdditionalReferenceExport(withsenderinfo)] in Appendix. Although these gains are marginal, it is interesting to note that there still was room for improvement. 
Similiar results can also be seen for import transport orders see Tables \ref[tableAdditionalReferenceImport(withsenderinfo)] and \ref[tableAdditionalReferenceImport]. No futher improvements have been tried as these seems to be limits of reasonably interpretable and meaningful measurement.
\midinsert \clabel[tableAdditionalReferenceExport]{Model ComparisonAdditional Reference Export}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
MD:rf & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:lgbm & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:rf & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:LogRegression & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:xgb & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:KerasNNClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:KNeighborsClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:VotingClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:LogRegression & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:sgdLogLoss & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:sgdModifiedHuber & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:KNeighborsClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:VotingClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:BernoulliNB & 0.500 & 0.000 & 0.500 & 0.000 & 0.999 & 0.999 & 0.999 & 0.999 \cr
FBOD-preprocess & 0.500 & 0.001 & 0.499 & 0.000 & 0.999 & 1.000 & 0.998 & 0.999 \cr
FBODBackOff-preprocess & 0.500 & 0.001 & 0.499 & 0.000 & 0.999 & 1.000 & 0.998 & 0.999 \cr
FBOD & 0.500 & 0.001 & 0.499 & 0.000 & 0.998 & 1.000 & 0.997 & 0.998 \cr
FBODBackOff & 0.500 & 0.001 & 0.499 & 0.000 & 0.998 & 1.000 & 0.997 & 0.998 \cr
CBAD:MultinomialNB & 0.499 & 0.001 & 0.499 & 0.001 & 0.998 & 0.998 & 0.998 & 0.998 \cr
CBAD:ComplementNB & 0.458 & 0.043 & 0.457 & 0.042 & 0.915 & 0.915 & 0.915 & 0.915 \cr
}
\caption/t Performance metrics per model in Additional Reference Export (sorted by F1)
\endinsert

\sec Temperature

\midinsert \clabel[tableTEMPERATUREwithNULLS]{Model ComparisonTEMPERATURE with NULLS}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
FBODBackOff-preprocess & 0.500 & 0.015 & 0.485 & 0.000 & 0.984 & 0.999 & 0.970 & 0.984 \cr
FBOD & 0.499 & 0.016 & 0.484 & 0.001 & 0.984 & 0.999 & 0.970 & 0.984 \cr
FBOD-preprocess & 0.499 & 0.016 & 0.484 & 0.001 & 0.984 & 0.998 & 0.970 & 0.984 \cr
FBODBackOff & 0.498 & 0.015 & 0.485 & 0.002 & 0.982 & 0.996 & 0.970 & 0.983 \cr
MD:VotingClassifier & 0.500 & 0.179 & 0.321 & 0.000 & 0.821 & 1.000 & 0.737 & 0.848 \cr
MD:KerasNNClassifier & 0.500 & 0.179 & 0.321 & 0.000 & 0.821 & 1.000 & 0.737 & 0.848 \cr
MD:LogRegression & 0.500 & 0.179 & 0.321 & 0.000 & 0.821 & 1.000 & 0.736 & 0.848 \cr
CBAD:sgdModifiedHuber & 0.499 & 0.180 & 0.320 & 0.001 & 0.820 & 0.999 & 0.735 & 0.847 \cr
MD:rf & 0.496 & 0.179 & 0.321 & 0.004 & 0.817 & 0.992 & 0.735 & 0.845 \cr
CBAD:LogRegression & 0.497 & 0.180 & 0.320 & 0.003 & 0.817 & 0.994 & 0.734 & 0.844 \cr
CBAD:sgdLogLoss & 0.500 & 0.186 & 0.314 & 0.000 & 0.814 & 1.000 & 0.728 & 0.843 \cr
CBAD:VotingClassifier & 0.499 & 0.191 & 0.309 & 0.001 & 0.808 & 0.998 & 0.723 & 0.839 \cr
CBAD:rf & 0.488 & 0.180 & 0.320 & 0.012 & 0.808 & 0.976 & 0.731 & 0.836 \cr
MD:lgbm & 0.500 & 0.215 & 0.285 & 0.000 & 0.785 & 1.000 & 0.700 & 0.823 \cr
CBAD:MultinomialNB & 0.500 & 0.218 & 0.282 & 0.000 & 0.782 & 1.000 & 0.697 & 0.821 \cr
CBAD:BernoulliNB & 0.495 & 0.225 & 0.275 & 0.005 & 0.770 & 0.989 & 0.688 & 0.811 \cr
MD:KNeighborsClassifier & 0.500 & 0.278 & 0.222 & 0.000 & 0.722 & 1.000 & 0.643 & 0.782 \cr
CBAD:KNeighborsClassifier & 0.500 & 0.282 & 0.218 & 0.000 & 0.718 & 0.999 & 0.639 & 0.780 \cr
CBAD:ComplementNB & 0.500 & 0.445 & 0.055 & 0.000 & 0.554 & 1.000 & 0.529 & 0.692 \cr
}
\caption/t Performance metrics per model in TEMPERATURE with NULLS (sorted by F1)
\endinsert

\midinsert \clabel[tableTEMPERATUREwithNULLSCBAD]{Model threshold Comparison TEMPERATURE with NULLs }
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Acc & Rec & Prec & F1 \crli \tskip4pt
CBAD(t=0.1):rf & 0.500 & 0.015 & 0.485 & 0.000 & 0.984 & 0.999 & 0.970 & 0.985 \cr
FBODBackOff-preprocess & 0.500 & 0.015 & 0.485 & 0.000 & 0.984 & 0.999 & 0.970 & 0.984 \cr
FBOD & 0.499 & 0.016 & 0.484 & 0.001 & 0.984 & 0.999 & 0.970 & 0.984 \cr
FBOD-preprocess & 0.499 & 0.016 & 0.484 & 0.001 & 0.984 & 0.998 & 0.970 & 0.984 \cr
FBODBackOff & 0.498 & 0.015 & 0.485 & 0.002 & 0.982 & 0.996 & 0.970 & 0.983 \cr
CBAD(t=0.1):KNeighborsClassifier & 0.488 & 0.016 & 0.484 & 0.012 & 0.972 & 0.975 & 0.969 & 0.972 \cr
CBAD(t=0.1):LogRegression & 0.487 & 0.018 & 0.482 & 0.013 & 0.969 & 0.975 & 0.964 & 0.969 \cr
CBAD(t=0.1):sgdLogLoss & 0.477 & 0.016 & 0.484 & 0.023 & 0.961 & 0.954 & 0.967 & 0.960 \cr
CBAD(t=0.1):MultinomialNB & 0.499 & 0.057 & 0.443 & 0.001 & 0.942 & 0.998 & 0.898 & 0.945 \cr
CBAD:sgdModifiedHuber & 0.499 & 0.180 & 0.320 & 0.001 & 0.820 & 0.999 & 0.735 & 0.847 \cr
CBAD:LogRegression & 0.497 & 0.180 & 0.320 & 0.003 & 0.817 & 0.994 & 0.734 & 0.844 \cr
CBAD:sgdLogLoss & 0.500 & 0.186 & 0.314 & 0.000 & 0.814 & 1.000 & 0.728 & 0.843 \cr
CBAD:VotingClassifier & 0.499 & 0.191 & 0.309 & 0.001 & 0.808 & 0.998 & 0.723 & 0.839 \cr
CBAD:rf & 0.488 & 0.180 & 0.320 & 0.012 & 0.808 & 0.976 & 0.731 & 0.836 \cr
CBAD:MultinomialNB & 0.500 & 0.218 & 0.282 & 0.000 & 0.782 & 1.000 & 0.697 & 0.821 \cr
CBAD:BernoulliNB & 0.495 & 0.225 & 0.275 & 0.005 & 0.770 & 0.989 & 0.688 & 0.811 \cr
CBAD(t=0.9):BernoulliNB & 0.495 & 0.226 & 0.274 & 0.005 & 0.769 & 0.989 & 0.686 & 0.810 \cr
CBAD(t=0.1):BernoulliNB & 0.491 & 0.221 & 0.279 & 0.009 & 0.770 & 0.982 & 0.690 & 0.810 \cr
CBAD(t=0.1):VotingClassifier & 0.342 & 0.015 & 0.485 & 0.158 & 0.827 & 0.684 & 0.959 & 0.799 \cr
CBAD:KNeighborsClassifier & 0.500 & 0.282 & 0.218 & 0.000 & 0.718 & 0.999 & 0.639 & 0.780 \cr
CBAD(t=0.1):ComplementNB & 0.499 & 0.440 & 0.060 & 0.001 & 0.559 & 0.998 & 0.531 & 0.694 \cr
CBAD:ComplementNB & 0.500 & 0.445 & 0.055 & 0.000 & 0.554 & 1.000 & 0.529 & 0.692 \cr
CBAD(t=0.9):ComplementNB & 0.500 & 0.455 & 0.045 & 0.000 & 0.545 & 1.000 & 0.524 & 0.687 \cr
CBAD(t=0.9):KNeighborsClassifier & 0.500 & 0.465 & 0.035 & 0.000 & 0.535 & 1.000 & 0.518 & 0.683 \cr
CBAD(t=0.9):sgdModifiedHuber & 0.500 & 0.470 & 0.030 & 0.000 & 0.530 & 1.000 & 0.515 & 0.680 \cr
CBAD(t=0.9):LogRegression & 0.500 & 0.473 & 0.027 & 0.000 & 0.527 & 1.000 & 0.514 & 0.679 \cr
CBAD(t=0.9):MultinomialNB & 0.500 & 0.474 & 0.026 & 0.000 & 0.526 & 1.000 & 0.513 & 0.678 \cr
CBAD(t=0.9):rf & 0.497 & 0.468 & 0.032 & 0.003 & 0.529 & 0.994 & 0.515 & 0.678 \cr
CBAD(t=0.9):VotingClassifier & 0.500 & 0.486 & 0.014 & 0.000 & 0.513 & 0.999 & 0.507 & 0.673 \cr
CBAD(t=0.9):sgdLogLoss & 0.500 & 0.491 & 0.009 & 0.000 & 0.509 & 0.999 & 0.504 & 0.670 \cr
CBAD(t=0.1):sgdModifiedHuber & 0.225 & 0.003 & 0.497 & 0.275 & 0.722 & 0.449 & 0.987 & 0.617 \cr
}
\caption/t Performance metrics per model in TEMPERATURE with NULLS, CBAD with different thresholds (sorted by F1)
\endinsert

\midinsert \clabel[tableTEMPERATURE]{Model Comparison TEMPERATURE}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Acc & Rec & Prec & F1 \crli \tskip4pt
CBAD(t=0.1):sgdModifiedHuber & 0.500 & 0.004 & 0.496 & 0.000 & 0.995 & 0.999 & 0.992 & 0.995 \cr
CBAD(t=0.1):KNeighborsClassifier & 0.500 & 0.024 & 0.476 & 0.000 & 0.976 & 1.000 & 0.954 & 0.976 \cr
FBODBackOff-preprocess & 0.500 & 0.024 & 0.476 & 0.000 & 0.975 & 1.000 & 0.953 & 0.976 \cr
CBAD(t=0.1):VotingClassifier & 0.499 & 0.024 & 0.476 & 0.001 & 0.975 & 0.998 & 0.954 & 0.976 \cr
CBAD(t=0.1):LogRegression & 0.500 & 0.025 & 0.475 & 0.000 & 0.975 & 0.999 & 0.953 & 0.975 \cr
FBOD-preprocess & 0.500 & 0.025 & 0.475 & 0.000 & 0.975 & 0.999 & 0.953 & 0.975 \cr
CBAD(t=0.1):rf & 0.499 & 0.024 & 0.476 & 0.001 & 0.975 & 0.998 & 0.953 & 0.975 \cr
FBOD & 0.500 & 0.025 & 0.475 & 0.000 & 0.975 & 0.999 & 0.953 & 0.975 \cr
CBAD(t=0.1):sgdLogLoss & 0.500 & 0.026 & 0.474 & 0.000 & 0.974 & 0.999 & 0.951 & 0.975 \cr
MD:VotingClassifier & 0.500 & 0.028 & 0.472 & 0.000 & 0.972 & 1.000 & 0.947 & 0.973 \cr
MD:rf & 0.500 & 0.028 & 0.472 & 0.000 & 0.972 & 1.000 & 0.947 & 0.973 \cr
MD:KerasNNClassifier & 0.500 & 0.028 & 0.472 & 0.000 & 0.972 & 1.000 & 0.947 & 0.973 \cr
MD:KNeighborsClassifier & 0.500 & 0.028 & 0.472 & 0.000 & 0.972 & 1.000 & 0.947 & 0.973 \cr
MD:LogRegression & 0.498 & 0.028 & 0.472 & 0.002 & 0.970 & 0.996 & 0.947 & 0.971 \cr
CBAD(t=0.1):MultinomialNB & 0.499 & 0.033 & 0.467 & 0.001 & 0.966 & 0.997 & 0.939 & 0.967 \cr
FBODBackOff & 0.489 & 0.024 & 0.476 & 0.011 & 0.964 & 0.978 & 0.952 & 0.965 \cr
CBAD(t=0.1):ComplementNB & 0.499 & 0.042 & 0.458 & 0.001 & 0.956 & 0.997 & 0.922 & 0.958 \cr
CBAD(t=0.9):MultinomialNB & 0.500 & 0.048 & 0.452 & 0.000 & 0.952 & 1.000 & 0.912 & 0.954 \cr
CBAD(t=0.9):KNeighborsClassifier & 0.500 & 0.051 & 0.449 & 0.000 & 0.949 & 1.000 & 0.908 & 0.952 \cr
CBAD(t=0.9):BernoulliNB & 0.500 & 0.051 & 0.449 & 0.000 & 0.949 & 0.999 & 0.908 & 0.951 \cr
CBAD(t=0.9):LogRegression & 0.500 & 0.052 & 0.448 & 0.000 & 0.948 & 1.000 & 0.907 & 0.951 \cr
CBAD(t=0.9):VotingClassifier & 0.500 & 0.061 & 0.439 & 0.000 & 0.939 & 1.000 & 0.891 & 0.942 \cr
CBAD(t=0.1):BernoulliNB & 0.478 & 0.047 & 0.453 & 0.022 & 0.932 & 0.957 & 0.911 & 0.933 \cr
CBAD(t=0.9):rf & 0.478 & 0.047 & 0.453 & 0.022 & 0.931 & 0.956 & 0.910 & 0.932 \cr
CBAD(t=0.9):sgdLogLoss & 0.500 & 0.075 & 0.425 & 0.000 & 0.925 & 1.000 & 0.870 & 0.930 \cr
MD:lgbm & 0.500 & 0.078 & 0.422 & 0.000 & 0.922 & 1.000 & 0.865 & 0.927 \cr
CBAD(t=0.9):sgdModifiedHuber & 0.500 & 0.461 & 0.039 & 0.000 & 0.539 & 1.000 & 0.520 & 0.685 \cr
CBAD(t=0.9):ComplementNB & 0.500 & 0.470 & 0.030 & 0.000 & 0.530 & 1.000 & 0.516 & 0.680 \cr
}
\caption/t Performance metrics per model in TEMPERATURE (sorted by F1)
\endinsert

\midinsert \clabel[tableTEMPERATUREwithinfo]{Model Comparison TEMPERATURE with info}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Acc & Rec & Prec & F1 \crli \tskip4pt
CBAD(t=0.1):LogRegression & 0.500 & 0.006 & 0.494 & 0.000 & 0.994 & 1.000 & 0.989 & 0.994 \cr
CBAD(t=0.1):sgdModifiedHuber & 0.500 & 0.006 & 0.494 & 0.000 & 0.994 & 1.000 & 0.988 & 0.994 \cr
FBODBackOff-preprocess & 0.499 & 0.006 & 0.494 & 0.001 & 0.993 & 0.998 & 0.988 & 0.993 \cr
FBOD & 0.499 & 0.009 & 0.491 & 0.001 & 0.991 & 0.999 & 0.983 & 0.991 \cr
CBAD(t=0.1):sgdLogLoss & 0.497 & 0.006 & 0.494 & 0.003 & 0.991 & 0.994 & 0.988 & 0.991 \cr
FBODBackOff & 0.496 & 0.006 & 0.494 & 0.004 & 0.991 & 0.993 & 0.988 & 0.991 \cr
FBOD-preprocess & 0.497 & 0.008 & 0.492 & 0.003 & 0.990 & 0.994 & 0.985 & 0.990 \cr
CBAD(t=0.1):MultinomialNB & 0.500 & 0.025 & 0.475 & 0.000 & 0.975 & 0.999 & 0.952 & 0.975 \cr
MD:LogRegression & 0.500 & 0.027 & 0.473 & 0.000 & 0.973 & 1.000 & 0.950 & 0.974 \cr
MD:rf & 0.500 & 0.027 & 0.473 & 0.000 & 0.973 & 1.000 & 0.950 & 0.974 \cr
MD:KerasNNClassifier & 0.500 & 0.027 & 0.473 & 0.000 & 0.973 & 1.000 & 0.948 & 0.973 \cr
MD:KNeighborsClassifier & 0.499 & 0.027 & 0.473 & 0.001 & 0.972 & 0.999 & 0.948 & 0.973 \cr
CBAD(t=0.1):VotingClassifier & 0.478 & 0.005 & 0.495 & 0.022 & 0.973 & 0.957 & 0.989 & 0.973 \cr
CBAD(t=0.1):rf & 0.478 & 0.006 & 0.494 & 0.022 & 0.973 & 0.957 & 0.988 & 0.972 \cr
CBAD(t=0.1):ComplementNB & 0.500 & 0.039 & 0.461 & 0.000 & 0.961 & 1.000 & 0.928 & 0.962 \cr
CBAD(t=0.9):KNeighborsClassifier & 0.500 & 0.039 & 0.461 & 0.000 & 0.961 & 1.000 & 0.927 & 0.962 \cr
CBAD(t=0.9):ComplementNB & 0.500 & 0.043 & 0.457 & 0.000 & 0.957 & 1.000 & 0.921 & 0.959 \cr
MD:VotingClassifier & 0.478 & 0.027 & 0.473 & 0.022 & 0.952 & 0.957 & 0.947 & 0.952 \cr
CBAD(t=0.1):KNeighborsClassifier & 0.475 & 0.024 & 0.476 & 0.025 & 0.951 & 0.950 & 0.952 & 0.951 \cr
MD:lgbm & 0.499 & 0.054 & 0.446 & 0.001 & 0.946 & 0.999 & 0.903 & 0.949 \cr
CBAD(t=0.1):BernoulliNB & 0.499 & 0.114 & 0.386 & 0.001 & 0.886 & 0.998 & 0.815 & 0.897 \cr
CBAD(t=0.9):sgdModifiedHuber & 0.500 & 0.125 & 0.375 & 0.000 & 0.875 & 1.000 & 0.801 & 0.889 \cr
CBAD(t=0.9):rf & 0.500 & 0.125 & 0.375 & 0.000 & 0.875 & 1.000 & 0.799 & 0.889 \cr
CBAD(t=0.9):LogRegression & 0.500 & 0.129 & 0.371 & 0.000 & 0.871 & 1.000 & 0.794 & 0.885 \cr
CBAD(t=0.9):MultinomialNB & 0.500 & 0.132 & 0.368 & 0.000 & 0.868 & 1.000 & 0.791 & 0.884 \cr
CBAD(t=0.9):VotingClassifier & 0.500 & 0.140 & 0.360 & 0.000 & 0.860 & 1.000 & 0.781 & 0.877 \cr
CBAD(t=0.9):BernoulliNB & 0.500 & 0.140 & 0.360 & 0.000 & 0.860 & 1.000 & 0.781 & 0.877 \cr
CBAD(t=0.9):sgdLogLoss & 0.500 & 0.144 & 0.356 & 0.000 & 0.856 & 1.000 & 0.776 & 0.874 \cr
}
\caption/t Performance metrics per model in TEMPERATURE with info (sorted by F1)
\endinsert


\sec Discussion