% !TEX program = optex
% !TEX root = ambrojak-thesis-masters.tex
\label[results]
\chap Evaluation of Detection Models

This chapter starts with the evaluation of anomaly detection models, adopting performance metrics from binary classification theory. The approach involves comparing model predictions against ground-truth labels and visualizing trade-offs as score thresholds are adjusted. Key metrics such as the confusion matrix, precision, recall, and F1 score are discussed, alongside visualization techniques.

Afterwards follows a section with results where we compare different models on the anomlies defined at the beggining of this thesis. And we end the chapter with dicussion of the results.

\sec Evaluation methods
Modern anomaly detection systems borrow their performance metrics from binary classification theory: they compare the predictions of the model (or anomaly scores) with ground-truth labels and visualize the trade-offs that arise when a score threshold is moved. Below is a concise overview of the core tabular metrics—confusion matrix, precision, recall and F1—and the threshold-sweeping visualisations.

\secc Confusion Matrix
The confusion matrix is a 2×2 contingency table that enumerates these four outcomes and forms the basis for almost every downstream metric.\cite[mediumConfusionMatrix]

\midinsert \clabel[confusionMatrixTable]{Confusion Matrix}
\ctable{l|cc}{\hfil
Actual/Prediction  & Predicted Positive & Predictive Negative \crli \tskip4pt
Actual Positive & True Positive (TP) & False Negative (FN) \cr
Actual Negative & False Positive (FP) & True Negative (TN) \cr
}
\caption/t Table explaining Confusion Matrix
\endinsert
{\bf In this thesis we always view the anomalous class as positive.} The negative is therefore referring to normal (nonanomalous) data. In other words, we are asking the question ``Is this datum anomalous?''

The table \ref[confusionMatrixTable] is explaining TP, FP, FN and TN. And an example of how to use them to compare several models can be seen in Figure \ref[confusionMatrixFigure]

\medskip
\clabel[confusionMatrixFigure]{Confusion Matrix for models}
\picw=15cm \cinspic imgs/ConfusionMatrix.pdf
\caption/f Confusion matrix for FBOD models (on Customs Anomaly)
\medskip

Also there is a need to mention Type I and Type II errors. Type I error is an alternative name for FP (False Positive). And Type II error is a synonym for FN (False Negative) \cite[Chadha2020DistilledEvaluationMetrics].

They are, in my opinion, unnecessarily confusing names. And there is no need to mention them anywhere else in this thesis, as the terms FP and FN are sufficient, and furthermore, their names already hint at their meaning.

\secc Accuracy
The simplest way to measure the performance of the model is to measure the frequency with which the prediction of the model matches the actual class. This measure may hide crucial information when one class is very rare \cite[Chadha2020DistilledEvaluationMetrics].

This is the case with anomalous data that were provided by Metrans. Although this can be mitigated by synthetic anomaly generation that can even out the distribution of the classes and make this measure once again somewhat useful.

$$\rm
{Accuracy} = {TP + TN \over TP + TN + FP + FN} \eqmark[accuracy]
$$

\label[precision]
\secc Precision
Precision (Positive Predictive Value, PPV) quantifies the purity of the detected anomalies \cite[mediumConfusionMatrix]. It answers the question ``When detector predicts that the datum is positive (anomalous), how often is it right?''

$$\rm
{Precision} = {TP \over TP + FP} \eqmark[eq-precision]
$$

% $$
% {Precision} = {TP \over TP + FP}
% $$
% $$\it
% {Precision} = {TP \over TP + FP}
% $$
% $$\cal
% {PRECISIONrecision} = {TP \over TP + FP}
% $$
% $$\mit
% {Precision} = {TP \over TP + FP}
% $$
% $$\bf
% {Precision} = {TP \over TP + FP}
% $$

\label[recall]
\secc Recall
Recall (Sensitivity, True Positive Rate, TPR) measures completeness\cite[mediumConfusionMatrix]. It answers ``How many real anomalies did we catch?''
$$\rm
{Recall} = {TP \over TP + FN} \eqmark[eq-recall]
$$

\label[specificity]
\secc Specificity
Specificity (True Negative Rate, TNR) measures the proportion of actual negatives correctly identified \cite[Chadha2020DistilledEvaluationMetrics]. It answers the question ``How good are we at detecting OK data?''.

This metric is not very useful for our case. We care about detecting anomalies, as not detecting them is costly. Detecting nonanomalous data is not crucial. This metric is usefull for minimizing false alarms. But in our case these are not very costly.
$$\rm
{Specificity} = {TN \over  TN + FP} \eqmark[eq-specificity]
$$

\label[npv]
\secc Negative Predictive Value (NPV):
Negative Predictive Value (NPV) represents the probability that instances predicted as normal are truly normal \cite[Chadha2020DistilledEvaluationMetrics]. It answers the question ``If we predict that datum is OK, how likely is it actually OK?''.

This metric is again not very useful for our case. A high NPV suggests confidence in the model's negative predictions, ensuring that normal instances are not misclassified as anomalies. And this once again is not that costly of a mistake in our case.
$$\rm
{NPV} = {TN \over TN + FN} \eqmark[npv]
$$

\secc F1 Score
F1 score is the harmonic mean Precision \ref[precision] and Recall \ref[recall]. It gives a single number that penalizes detectors that are skewed toward either high precision or toward high recall \cite[mediumConfusionMatrix].

This metric is very usefull in our case as it focuses on detecting anomalies. But it also gives a singular number that allows us to evaluate our models and rank them from best to worst. For an example graph for comparison of several models using F1 (and Precision and Recall), see Figure \ref[FigureF1].

$$\rm
{F1} = 2 \times { Precision \times Recall \over Precision + Recall} \eqmark[eq-f1]
$$

\medskip
\clabel[FigureF1]{Typical metrics for model comparsion}
\picw=15cm \cinspic imgs/Metrics.pdf
\caption/f Typical metrics for anomaly detection used for model comparison
\medskip

\sec Visualization Techniques
We have also tried several visualization techniques to provide better insight into the results. These techniques use Anomaly scores, that is (for most models) a probability score of the datum being an anomaly.

\secc Anomaly Score Distribution

An anomaly‐score distribution graph displays how the computed scores for each data point—whether normal or anomalous—are spread across the range of possible values. Typically, you’ll see two overlapping curves or histograms: one representing the bulk of “normal” orders clustered at lower scores, and a sparser tail or separate peak at higher scores corresponding to the few truly anomalous entries. By visualizing these distributions side by side, you can choose a threshold score that best separates the two populations, minimizing the chance of misclassifying a normal order as anomalous while still catching the majority of true anomalies. The degree of overlap between the curves indicates how difficult the detection problem is: the less overlap, the more confidently you can distinguish outliers from routine orders.

\medskip
\clabel[AnomalyScore]{Anomaly Score Distribtion}
\picw=15cm \cinspic imgs/AnomalyScores.pdf
\caption/f Distrbution of anomaly scrores by Voting Classifier Heating Type Anomaly (appears to be easily separable)
\medskip

\label[ROC]
\secc Reciever Operating Characteristic
A {\bf Receiver Operating Characteristic (ROC)} curve traces how a classifier’s true positive rate changes as its false positive rate increases when you slide the decision threshold from strict to permissive. At each threshold, the true positive rate (also called sensitivity) measures the proportion of actual positives correctly identified, while the false positive rate measures the proportion of actual negatives that are incorrectly labeled as positives. Plotting the false positive rate on the x-axis against the true positive rate on the y-axis yields a curve that starts at the origin (no positives predicted) and ends at the point (1, 1) (all instances predicted positive). A ROC curve that bows sharply toward the top-left corner indicates that the model achieves high sensitivity while keeping the false alarm rate low across a broad range of thresholds; by contrast, a curve that clings close to the diagonal line reflects a model whose predictions are barely better than random guessing.\cite[murphy2012machine]

\medskip
\clabel[ROCCurve]{ROC Curve}
\picw=10cm \cinspic imgs/ROCcurve.pdf
\caption/f Example of Reciever Operating Characteristic Curve for Voting Classifier (Customs Anomaly)
\medskip

The {\bf Area Under the ROC curve} (often abbreviated as {\bf AUC-ROC}) provides a single-number summary of classification performance by quantifying the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative one. An AUC of 1.0 represents a perfect classifier that assigns higher scores to all positives than to all negatives, while an AUC of 0.5 corresponds to performance no better than chance. Because the ROC AUC considers both types of errors equally, it remains largely insensitive to changes in class balance; however, in scenarios where false positives carry a heavier cost than false negatives (or vice versa), one may still need to select an operating threshold that reflects the desired trade-off, using the ROC curve to identify the point where the increase in true positive rate no longer justifies the rise in false positives.\cite[murphy2012machine]

\label[PRC]
\secc Precision Recall Curve
A precision-recall (PR Curve, PRC) curve visualizes the trade-off between precision (section \ref[precision]) and recall (section \ref[recall]) for different decision thresholds of a probabilistic classifier. An example can be seen in figure \ref[PRCurve]. And the process of constructing in is outlined below
\begitems
* Compute scores. The classifier assigns each instance a score or probability of being positive (e.g., anomaly score, probability of ``anomalous'' class).

* Vary the threshold. For each threshold $t$ in $[0,1]$. Label as “positive” all instances with score $≥t$, others as “negative.”

* Compute the precision and recall

* Plot On the x-axis: recall; on the y-axis: precision. Connecting the points as the threshold moves from high (very strict) to low (very lenient) yields the PR curve.
\enditems

\medskip
\clabel[PRCurve]{Precision Recall Curve}
\picw=10cm \cinspic imgs/Precision-Recall.pdf
\caption/f Example of precision recall curve for Voting Classifier (Customs Anomaly)
\medskip

When examining a precision–recall curve, the ideal situation is represented by a point at the top-right corner, where both precision and recall equal 1, meaning every positive instance is found and there are no false positives. If the curve sits toward the upper-left region, it indicates that the classifier is highly precise but has low recall—it only labels as positive those instances it is extremely confident about, and consequently misses many true positives. Conversely, when the curve bulges toward the lower-right region, it reflects high recall but low precision, implying that although most actual positives are flagged, a large number of negative instances are incorrectly classified as positive. The overall shape of the curve matters: a curve that hugs the top-right corner demonstrates strong performance, whereas a sharp drop in precision for only a slight gain in recall suggests that lowering the threshold adds mostly false positives without substantially improving detection of true positives.\cite[murphy2012machine]

The {\bf Area Under the Precision–Recall Curve (AUPRC)} provides a single-number summary of this trade-off across all thresholds. Because it focuses exclusively on the behavior of the classifier with respect to the positive class, AUPRC is especially informative when positive examples are rare. In contrast to the ROC AUC, which averages performance in both classes and can appear deceptively high if the negative class dominates, the AUPRC more accurately reflects a model’s ability to find positive cases while avoiding false alarms in imbalanced settings. \cite[murphy2012machine]

\label[secResults]
\sec Results
In the following subsections we look into how well and what models performed on each anomaly type. We provide several tables that serve as a basis to discuss each anomaly type. Some tables are located in the Appendix \ref[app-tables] to at least slightly improve the redibility of this section.

For all anomalies, we have used $200k$ transport orders from the archives. We have split of 10\% to use as testing data. And for anomalous data we have altered these testing data by switching target labels and afterwards merged them with original testing data.

\secc Heating Type
The performance of various models can be seen in \ref[tableHeatingTypeAnomalyBoW]. The best models reach F1 scores over $0.99$. In this table we have used our preprocessing methods, applied BoW vecotorization and used information about client, reciever, goods and chassis type.

In several more tables we have tried to decompose the major sources of our success. First of all we have removed our preprocessing (see Table \ref[tableHeatingTypeAnomalyNoPreprocess]). Preprocessing seems to have made some improvement, but not much. I suspect that it is because internally BoW or TF-IDF provided by sklearn does the most crucial parts of preprocessing by itself (e.g. case unification). But it seem to have some small improvements for. At least FBOD models, see Table \ref[tableComparepreprocessingFBOD].

Second, we have tried switching from BoW to TF-IDF (see Table \ref[tableHeatingTypeAnomalyGoodsChassis]). Once again, the TF-IDF version is a bit worse but not a significant ammount. The best improvement over the baseline model \ref[tableHeatingTypeAnomaly] seems to be adding more information about the order. The information on both chassis and goods has helped, although the goods seem to be the more important (see Table \ref[tableHeatingTypeAnomalyGoods]).

Overall even the performance of baseline models was very good. That is not suprising as further look into anomaly scores (see Figure \ref[AnomalyScore]) shows us that for most models the difference between anomalous and nonanomlaous data is significant. Despite that we have made some further Accuracy (and F1 score) gains using better preprocessing, vectorization and most importantly adding more info about the transportation order.
The models seem to have been Random Forests (as were expected) but are closely followed by CBAD with even unexpect estimators such as LogRegression or SGD.

\midinsert \clabel[tableHeatingTypeAnomalyBoW]{Model Comparison Heating Type Anomaly BoW}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
MD:rf & 0.498 & 0.008 & 0.492 & 0.002 & 0.990 & 0.996 & 0.984 & 0.990 \cr
CBAD:rf & 0.498 & 0.008 & 0.492 & 0.002 & 0.990 & 0.996 & 0.983 & 0.990 \cr
CBAD:sgdModifiedHuber & 0.497 & 0.009 & 0.491 & 0.003 & 0.989 & 0.995 & 0.983 & 0.989 \cr
CBAD:LogRegression & 0.498 & 0.009 & 0.491 & 0.002 & 0.989 & 0.995 & 0.983 & 0.989 \cr
MD:KNeighborsClassifier & 0.497 & 0.009 & 0.491 & 0.003 & 0.988 & 0.994 & 0.982 & 0.988 \cr
MD:KerasNNClassifier & 0.495 & 0.009 & 0.491 & 0.005 & 0.986 & 0.990 & 0.982 & 0.986 \cr
MD:VotingClassifier & 0.494 & 0.008 & 0.492 & 0.006 & 0.986 & 0.988 & 0.984 & 0.986 \cr
MD:lgbm & 0.494 & 0.008 & 0.492 & 0.006 & 0.985 & 0.988 & 0.983 & 0.985 \cr
MD:xgb & 0.494 & 0.008 & 0.492 & 0.006 & 0.985 & 0.988 & 0.983 & 0.985 \cr
FBODBackOff & 0.488 & 0.003 & 0.497 & 0.012 & 0.986 & 0.977 & 0.995 & 0.985 \cr
MD:LogRegression & 0.494 & 0.009 & 0.491 & 0.006 & 0.985 & 0.988 & 0.983 & 0.985 \cr
CBAD:sgdLogLoss & 0.494 & 0.009 & 0.491 & 0.006 & 0.985 & 0.988 & 0.982 & 0.985 \cr
CBAD:KNeighborsClassifier & 0.494 & 0.009 & 0.491 & 0.006 & 0.984 & 0.988 & 0.981 & 0.985 \cr
FBOD & 0.489 & 0.006 & 0.494 & 0.011 & 0.984 & 0.979 & 0.989 & 0.984 \cr
CBAD:VotingClassifier & 0.494 & 0.011 & 0.489 & 0.006 & 0.982 & 0.987 & 0.977 & 0.982 \cr
CBAD:ComplementNB & 0.491 & 0.020 & 0.480 & 0.009 & 0.970 & 0.981 & 0.960 & 0.971 \cr
CBAD:MultinomialNB & 0.490 & 0.020 & 0.480 & 0.010 & 0.969 & 0.979 & 0.960 & 0.970 \cr
CBAD:BernoulliNB & 0.486 & 0.028 & 0.472 & 0.014 & 0.959 & 0.973 & 0.946 & 0.959 \cr
}
\caption/t Performance metrics per model in Heating Type Anomaly with information about goods and chassis type using BoW (sorted by F1)
\endinsert

\midinsert \clabel[tableComparepreprocessingFBOD]{Model Compare preprocessing FBOD}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
FBODBackOff-preprocess & 0.489 & 0.003 & 0.497 & 0.011 & 0.986 & 0.978 & 0.994 & 0.986 \cr
FBODBackOff & 0.488 & 0.003 & 0.497 & 0.012 & 0.986 & 0.977 & 0.995 & 0.985 \cr
FBOD-preprocess & 0.490 & 0.005 & 0.495 & 0.010 & 0.985 & 0.980 & 0.990 & 0.985 \cr
FBOD & 0.489 & 0.006 & 0.494 & 0.011 & 0.984 & 0.979 & 0.989 & 0.984 \cr
}
\caption/t Performance metrics in FBOD with and without preprocessing (sorted by F1)
\endinsert

\secc Temperature
Assumed to be one of the harder anomalies in our set, the temperature anomaly is split into three subtypes TEMPERATURE (heating medium temperature), TEMP_TO (upper bound on temperature of the goods) and TEMP_FROM (lower bound on the temperature of the goods).

We have treated each possible temperature as a separate category. With our preprocessing of the target column we were able to go from roughly 150 classes to around 30-70 classes\fnote{Depends on subtype, TEMP_TO and TEMP_FROM are around 70, while TEMPERATURE is at 30. This may explain better performance on the latter.} where the majority of cases (90\%+) were from a few (5) classes.

Thus, we also treated this as a categorical problem.\fnote{We will face similar issues in chassis type anyway, see \ref[chassis-results]}. Another issue was an abundance of NULL values. After discussing the current MIS3 database with Metrans, I have decided to omit them from the training (and testing) dataset, because in the current standard these values cannot be left blank and should always be filled out. \fnote{For those interested the performance on the models on the problem with NULLs see Table \ref[tableTEMPERATUREwithNULLS] or Table \ref[tableTEMPERATUREwithNULLSCBAD]}

In the data set without NULLs, we could reach a F1 score of $0.995$, see Table \ref[tableTEMPERATURE]. We have experimented with using purely "LabelEncoder" without our preprocessing and we have reached $0.994$, see Table \ref[tableTEMPERATURELabelEncoder]. After that we have used the promising tactic of using more information about the transport order - that is goods (description CZ/DE, HS Code) and chassis type, see Table \ref[tableTEMPERATUREwithinfo]. This is the most surprsing result as it did not only not improve but actively worsened our performance to F1 score of $0.994$.

The performances for TEMP_FROM (Table \ref[tableTEMP_FROM]) and TEMP_TO (Table \ref[tableTEMP_TO]) were a littler worse - $0.985$ and $0.967$ respectively.

Overall, we see that the decision to include FBOD and CBAD models has allowed us to surpass simple MD models. However the best classifier in CBAD were not only expected RF (Random Forest a tree-based method) but also SGD \fnote{This estimator implements regularized linear models with stochastic gradient descent \url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html}} or even simple Logistic Regression.


\midinsert \clabel[tableTEMPERATUREwithinfo]{Model Comparison TEMPERATURE with info}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Acc & Rec & Prec & F1 \crli \tskip4pt
CBAD(t=0.1):LogRegression & 0.500 & 0.006 & 0.494 & 0.000 & 0.994 & 1.000 & 0.989 & 0.994 \cr
CBAD(t=0.1):sgdModifiedHuber & 0.500 & 0.006 & 0.494 & 0.000 & 0.994 & 1.000 & 0.988 & 0.994 \cr
FBODBackOff-preprocess & 0.499 & 0.006 & 0.494 & 0.001 & 0.993 & 0.998 & 0.988 & 0.993 \cr
FBOD & 0.499 & 0.009 & 0.491 & 0.001 & 0.991 & 0.999 & 0.983 & 0.991 \cr
CBAD(t=0.1):sgdLogLoss & 0.497 & 0.006 & 0.494 & 0.003 & 0.991 & 0.994 & 0.988 & 0.991 \cr
FBODBackOff & 0.496 & 0.006 & 0.494 & 0.004 & 0.991 & 0.993 & 0.988 & 0.991 \cr
FBOD-preprocess & 0.497 & 0.008 & 0.492 & 0.003 & 0.990 & 0.994 & 0.985 & 0.990 \cr
CBAD(t=0.1):MultinomialNB & 0.500 & 0.025 & 0.475 & 0.000 & 0.975 & 0.999 & 0.952 & 0.975 \cr
MD:LogRegression & 0.500 & 0.027 & 0.473 & 0.000 & 0.973 & 1.000 & 0.950 & 0.974 \cr
MD:rf & 0.500 & 0.027 & 0.473 & 0.000 & 0.973 & 1.000 & 0.950 & 0.974 \cr
MD:KerasNNClassifier & 0.500 & 0.027 & 0.473 & 0.000 & 0.973 & 1.000 & 0.948 & 0.973 \cr
MD:KNeighborsClassifier & 0.499 & 0.027 & 0.473 & 0.001 & 0.972 & 0.999 & 0.948 & 0.973 \cr
CBAD(t=0.1):VotingClassifier & 0.478 & 0.005 & 0.495 & 0.022 & 0.973 & 0.957 & 0.989 & 0.973 \cr
CBAD(t=0.1):rf & 0.478 & 0.006 & 0.494 & 0.022 & 0.973 & 0.957 & 0.988 & 0.972 \cr
CBAD(t=0.1):ComplementNB & 0.500 & 0.039 & 0.461 & 0.000 & 0.961 & 1.000 & 0.928 & 0.962 \cr
CBAD(t=0.9):KNeighborsClassifier & 0.500 & 0.039 & 0.461 & 0.000 & 0.961 & 1.000 & 0.927 & 0.962 \cr
CBAD(t=0.9):ComplementNB & 0.500 & 0.043 & 0.457 & 0.000 & 0.957 & 1.000 & 0.921 & 0.959 \cr
MD:VotingClassifier & 0.478 & 0.027 & 0.473 & 0.022 & 0.952 & 0.957 & 0.947 & 0.952 \cr
CBAD(t=0.1):KNeighborsClassifier & 0.475 & 0.024 & 0.476 & 0.025 & 0.951 & 0.950 & 0.952 & 0.951 \cr
MD:lgbm & 0.499 & 0.054 & 0.446 & 0.001 & 0.946 & 0.999 & 0.903 & 0.949 \cr
CBAD(t=0.1):BernoulliNB & 0.499 & 0.114 & 0.386 & 0.001 & 0.886 & 0.998 & 0.815 & 0.897 \cr
CBAD(t=0.9):sgdModifiedHuber & 0.500 & 0.125 & 0.375 & 0.000 & 0.875 & 1.000 & 0.801 & 0.889 \cr
CBAD(t=0.9):rf & 0.500 & 0.125 & 0.375 & 0.000 & 0.875 & 1.000 & 0.799 & 0.889 \cr
CBAD(t=0.9):LogRegression & 0.500 & 0.129 & 0.371 & 0.000 & 0.871 & 1.000 & 0.794 & 0.885 \cr
CBAD(t=0.9):MultinomialNB & 0.500 & 0.132 & 0.368 & 0.000 & 0.868 & 1.000 & 0.791 & 0.884 \cr
CBAD(t=0.9):VotingClassifier & 0.500 & 0.140 & 0.360 & 0.000 & 0.860 & 1.000 & 0.781 & 0.877 \cr
CBAD(t=0.9):BernoulliNB & 0.500 & 0.140 & 0.360 & 0.000 & 0.860 & 1.000 & 0.781 & 0.877 \cr
CBAD(t=0.9):sgdLogLoss & 0.500 & 0.144 & 0.356 & 0.000 & 0.856 & 1.000 & 0.776 & 0.874 \cr
}
\caption/t Performance metrics per model in TEMPERATURE with info (sorted by F1)
\endinsert


\secc Has Heating
We have tried several models, see table \ref[tableHasHeating]. The best models are reaching a point with units of errors per 10,000 orders. This makes sense as there are likely only a few companies in Central Europe that order tank heating. But this proves our decision to split of this anomaly from Heating type anomaly as important. As this anomaly is easier and would artificially improve our performance on the latter.
We do not try to adjust these models as this is already almost perfect performance.


\secc Customs number
We have tried several models to handle this anomaly, as can be seen in Table \ref[tableCustomsNumber]. Further improvements were made by adding more information about port destinations; see Table \ref[tableCustomsNumberPlace].

\midinsert \clabel[tableCustomsNumberPlace]{Model Comparison Customs number with place}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
MD:KerasNNClassifier & 0.496 & 0.004 & 0.496 & 0.004 & 0.992 & 0.992 & 0.992 & 0.992 \cr
MD:rf & 0.495 & 0.005 & 0.495 & 0.005 & 0.991 & 0.991 & 0.991 & 0.991 \cr
CBAD:rf & 0.495 & 0.005 & 0.495 & 0.005 & 0.991 & 0.991 & 0.991 & 0.991 \cr
MD:VotingClassifier & 0.495 & 0.005 & 0.495 & 0.005 & 0.991 & 0.991 & 0.991 & 0.991 \cr
CBAD:sgdModifiedHuber & 0.494 & 0.006 & 0.494 & 0.006 & 0.988 & 0.988 & 0.988 & 0.988 \cr
MD:LogRegression & 0.494 & 0.006 & 0.494 & 0.006 & 0.988 & 0.988 & 0.988 & 0.988 \cr
CBAD:LogRegression & 0.494 & 0.006 & 0.494 & 0.006 & 0.988 & 0.988 & 0.988 & 0.988 \cr
CBAD:VotingClassifier & 0.494 & 0.006 & 0.494 & 0.006 & 0.987 & 0.987 & 0.987 & 0.987 \cr
MD:KNeighborsClassifier & 0.494 & 0.006 & 0.494 & 0.006 & 0.987 & 0.987 & 0.987 & 0.987 \cr
CBAD:KNeighborsClassifier & 0.494 & 0.006 & 0.494 & 0.006 & 0.987 & 0.987 & 0.987 & 0.987 \cr
MD:lgbm & 0.493 & 0.007 & 0.493 & 0.007 & 0.986 & 0.986 & 0.986 & 0.986 \cr
MD:xgb & 0.492 & 0.008 & 0.492 & 0.008 & 0.985 & 0.985 & 0.985 & 0.985 \cr
FBODBackOff-preprocess & 0.486 & 0.002 & 0.498 & 0.014 & 0.984 & 0.973 & 0.996 & 0.984 \cr
FBODBackOff & 0.486 & 0.002 & 0.498 & 0.014 & 0.984 & 0.972 & 0.996 & 0.984 \cr
CBAD:sgdLogLoss & 0.492 & 0.008 & 0.492 & 0.008 & 0.984 & 0.984 & 0.984 & 0.984 \cr
FBOD-preprocess & 0.495 & 0.017 & 0.483 & 0.005 & 0.978 & 0.989 & 0.967 & 0.978 \cr
FBOD & 0.495 & 0.019 & 0.481 & 0.005 & 0.976 & 0.991 & 0.962 & 0.976 \cr
CBAD:MultinomialNB & 0.484 & 0.016 & 0.484 & 0.016 & 0.968 & 0.968 & 0.968 & 0.968 \cr
CBAD:BernoulliNB & 0.483 & 0.017 & 0.483 & 0.017 & 0.966 & 0.966 & 0.966 & 0.966 \cr
CBAD:ComplementNB & 0.481 & 0.019 & 0.481 & 0.019 & 0.961 & 0.961 & 0.961 & 0.961 \cr
}
\caption/t Performance metrics per model in Customs number with place (sorted by F1)
\endinsert

\secc Additional Billing Reference
This anomaly has proved trivial. Most reasonable models reached accuracy 99.95\% , so in table \ref[tableAdditionalReferenceExport] it appears as 100\% despite no model reaching that literally. This visualization uses only information about the client, adding also information about the receiver moves the accuracy past 99.97\%, even thought these improvements are rounded up we provide the Table \ref[tableAdditionalReferenceExport(withsenderinfo)] in Appendix. Although these gains are marginal, it is interesting to note that there still was room for improvement. 
Similiar results can also be seen for import transport orders see Tables \ref[tableAdditionalReferenceImport(withsenderinfo)] and \ref[tableAdditionalReferenceImport]. No futher improvements have been tried as these seems to be limits of reasonably interpretable and meaningful measurement.
\midinsert \clabel[tableAdditionalReferenceExport]{Model ComparisonAdditional Reference Export}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Accuracy & Recall & Precision & F1 \crli \tskip4pt
MD:rf & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:lgbm & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:rf & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:LogRegression & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:xgb & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:KerasNNClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:KNeighborsClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
MD:VotingClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:LogRegression & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:sgdLogLoss & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:sgdModifiedHuber & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:KNeighborsClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:VotingClassifier & 0.500 & 0.000 & 0.500 & 0.000 & 1.000 & 1.000 & 1.000 & 1.000 \cr
CBAD:BernoulliNB & 0.500 & 0.000 & 0.500 & 0.000 & 0.999 & 0.999 & 0.999 & 0.999 \cr
FBOD-preprocess & 0.500 & 0.001 & 0.499 & 0.000 & 0.999 & 1.000 & 0.998 & 0.999 \cr
FBODBackOff-preprocess & 0.500 & 0.001 & 0.499 & 0.000 & 0.999 & 1.000 & 0.998 & 0.999 \cr
FBOD & 0.500 & 0.001 & 0.499 & 0.000 & 0.998 & 1.000 & 0.997 & 0.998 \cr
FBODBackOff & 0.500 & 0.001 & 0.499 & 0.000 & 0.998 & 1.000 & 0.997 & 0.998 \cr
CBAD:MultinomialNB & 0.499 & 0.001 & 0.499 & 0.001 & 0.998 & 0.998 & 0.998 & 0.998 \cr
CBAD:ComplementNB & 0.458 & 0.043 & 0.457 & 0.042 & 0.915 & 0.915 & 0.915 & 0.915 \cr
}
\caption/t Performance metrics per model in Additional Reference Export (sorted by F1)
\endinsert

\label[chassis-results]
\secc Chassis Type
This has proved as probably the hardest anomaly. F1 score sits at $0.98$ on both import orders (Table \ref[tableChassisimport]) and export (Table \ref[tableChassisexport]). That is, however,  after filtering out NULL values (just as we did in the Temperature anomaly). In the version with missing values we are looking at F1 around $0.89$ (Table \ref[tableChassisimportwithNULLs]).
As we have expected simple MD models perform poorly (it  is possible that several container types are ok). And The topof these tables are FBOD and CBAD. For CBAD it is still necessary to choose an appropriate threshold.

And the more unexpected part are the models that perform on top for this anomaly such as SGD (once with LogLoss and once with ModifiedHuber), Logistic Regression or even voting classifiers, which have in previous anomalies performed roughly in the middle of the classifiers they are composed of.

\midinsert \clabel[tableChassisimport]{Model Comparison Chassis import}
\ctable{l|rrrrrrr|r}{
Model & TP & FP & TN & FN & Acc & Rec & Prec & F1 \crli \tskip4pt
CBAD(t=0.1):sgdLogLoss & 0.493 & 0.013 & 0.487 & 0.007 & 0.980 & 0.985 & 0.974 & 0.980 \cr
FBODBackOff & 0.491 & 0.028 & 0.472 & 0.009 & 0.963 & 0.981 & 0.946 & 0.963 \cr
CBAD(t=0.1):KNeighborsClassifier & 0.485 & 0.034 & 0.466 & 0.015 & 0.951 & 0.969 & 0.935 & 0.952 \cr
CBAD(t=0.1):BernoulliNB & 0.494 & 0.067 & 0.433 & 0.006 & 0.927 & 0.988 & 0.880 & 0.931 \cr
CBAD(t=0.1):sgdModifiedHuber & 0.420 & 0.019 & 0.481 & 0.080 & 0.901 & 0.840 & 0.957 & 0.894 \cr
CBAD(t=0.1):LogRegression & 0.420 & 0.020 & 0.480 & 0.080 & 0.899 & 0.839 & 0.954 & 0.893 \cr
FBOD-preprocess & 0.437 & 0.052 & 0.448 & 0.063 & 0.886 & 0.875 & 0.894 & 0.884 \cr
FullFBODBackOff & 0.400 & 0.028 & 0.472 & 0.100 & 0.872 & 0.800 & 0.934 & 0.862 \cr
FBODBackOff-preprocess & 0.395 & 0.027 & 0.473 & 0.105 & 0.868 & 0.789 & 0.936 & 0.856 \cr
MD:KerasNNClassifier & 0.498 & 0.186 & 0.314 & 0.002 & 0.812 & 0.996 & 0.728 & 0.841 \cr
MD:rf & 0.498 & 0.186 & 0.314 & 0.002 & 0.812 & 0.996 & 0.728 & 0.841 \cr
FullFBODBackOff-preprocess & 0.375 & 0.027 & 0.473 & 0.125 & 0.848 & 0.749 & 0.933 & 0.831 \cr
CBAD(t=0.1):rf & 0.368 & 0.025 & 0.475 & 0.132 & 0.843 & 0.736 & 0.936 & 0.824 \cr
MD:LogRegression & 0.486 & 0.195 & 0.305 & 0.014 & 0.791 & 0.972 & 0.714 & 0.823 \cr
MD:KNeighborsClassifier & 0.498 & 0.228 & 0.272 & 0.002 & 0.771 & 0.996 & 0.686 & 0.813 \cr
CBAD(t=0.1):VotingClassifier & 0.356 & 0.023 & 0.477 & 0.144 & 0.833 & 0.712 & 0.939 & 0.810 \cr
MD:VotingClassifier & 0.431 & 0.199 & 0.301 & 0.069 & 0.732 & 0.863 & 0.684 & 0.763 \cr
CBAD(t=0.1):ComplementNB & 0.432 & 0.214 & 0.286 & 0.068 & 0.718 & 0.863 & 0.669 & 0.754 \cr
CBAD(t=0.1):MultinomialNB & 0.342 & 0.065 & 0.435 & 0.158 & 0.777 & 0.684 & 0.840 & 0.754 \cr
CBAD(t=0.9):BernoulliNB & 0.488 & 0.363 & 0.137 & 0.012 & 0.625 & 0.977 & 0.574 & 0.723 \cr
FBOD & 0.316 & 0.058 & 0.442 & 0.184 & 0.758 & 0.631 & 0.845 & 0.723 \cr
CBAD(t=0.9):MultinomialNB & 0.482 & 0.365 & 0.135 & 0.018 & 0.618 & 0.965 & 0.569 & 0.716 \cr
CBAD(t=0.9):rf & 0.499 & 0.405 & 0.095 & 0.001 & 0.594 & 0.998 & 0.552 & 0.711 \cr
MD:lgbm & 0.494 & 0.398 & 0.102 & 0.006 & 0.597 & 0.988 & 0.554 & 0.710 \cr
CBAD(t=0.9):KNeighborsClassifier & 0.496 & 0.405 & 0.095 & 0.004 & 0.591 & 0.992 & 0.550 & 0.708 \cr
CBAD(t=0.9):LogRegression & 0.499 & 0.437 & 0.063 & 0.001 & 0.562 & 0.998 & 0.533 & 0.695 \cr
CBAD(t=0.9):sgdModifiedHuber & 0.499 & 0.439 & 0.061 & 0.001 & 0.560 & 0.999 & 0.532 & 0.694 \cr
CBAD(t=0.9):VotingClassifier & 0.500 & 0.444 & 0.056 & 0.000 & 0.556 & 1.000 & 0.530 & 0.692 \cr
CBAD(t=0.9):ComplementNB & 0.499 & 0.452 & 0.048 & 0.001 & 0.547 & 0.999 & 0.525 & 0.688 \cr
CBAD(t=0.9):sgdLogLoss & 0.500 & 0.476 & 0.024 & 0.000 & 0.524 & 0.999 & 0.512 & 0.677 \cr
}
\caption/t Performance metrics per model in Chassis import (sorted by F1)
\endinsert

\sec Discussion

The evaluation of models across different anomaly types reveals varied performance outcomes. For the Heating Type anomaly, the best models achieved F1 scores exceeding $0.99$. The analysis indicates that preprocessing, while helpful, did not contribute as significantly as the inclusion of additional information about the transport order, such as goods and chassis type. The overall strong performance for this anomaly is attributed to the significant difference in anomaly scores between anomalous and non-anomalous data for most models.

In contrast, the Temperature anomaly, initially considered harder, also showed high performance, with F1 scores reaching $0.995$ in datasets without NULL values. Interestingly, adding more information about the transport order, which proved beneficial for the Heating Type, did not improve performance for the Temperature anomaly and in some cases slightly worsened it. The Has Heating anomaly proved to be quite trivial, with most models achieving near-perfect accuracy. This supports the decision to evaluate it separately from the Heating Type anomaly.

The Customs number anomaly showed good performance, with further improvements observed when incorporating information about port destinations. The Additional Billing Reference anomaly was found to be trivial, with most models achieving very high accuracy, although marginal improvements were still possible by adding information about the receiver. Finally, the Chassis Type anomaly appears to be one of the most challenging, with F1 scores around $0.98$ after filtering out NULL values. The performance on this anomaly also highlighted that unexpected models like SGD and Logistic Regression performed well, alongside the anticipated FBOD models.